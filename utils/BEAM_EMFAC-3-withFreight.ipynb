{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13100387-c7ec-4f15-b3a8-26c60cded3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### AUTHOR: CRISTIAN POLIZIANI, PhD ################\n",
    "######## Lawrence Berkeley National Laboratory #############\n",
    "############################################################\n",
    "###################### May 2, 2024 #########################\n",
    "############################################################\n",
    "######SCOPE: Provide a BEAM CORE output link and ###########\n",
    "######## prepare pollutant emissions for INMAP #############\n",
    "############################################################\n",
    "\n",
    "# Match beam freight vehicles to the EMFAC\n",
    "# Add additional processes/pollutants for freight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdb9ea83-f442-4818-a0e4-cfcb894b6780",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################## READ #############################\n",
      "Read Files ... \n",
      "########################## NETWORK ##########################\n",
      " - Function:  split_network ...\n",
      " - Time taken: 0.00 seconds\n",
      " - Function:  calculate_midpoints ...\n",
      " - Time taken: 0.00 seconds\n",
      " - Function:  enrich_geo_data ...\n",
      " - Function:  addGeometryIdToDataFrame ...\n",
      " - Time taken: 1.90 seconds\n",
      " - Function:  addGeometryIdToDataFrame ...\n",
      " - Time taken: 8.18 seconds\n",
      "Santa Clara County      92524\n",
      "Alameda County          88520\n",
      "Contra Costa County     48082\n",
      "San Francisco County    45342\n",
      "San Mateo County        37538\n",
      "Sonoma County           26276\n",
      "Solano County           24880\n",
      "Marin County            13996\n",
      "Napa County              8774\n",
      "Name: county_name, dtype: int64\n",
      " - Function:  addGeometryIdToDataFrame ...\n",
      " - Time taken: 18.11 seconds\n",
      "Santa Clara      92524\n",
      "Alameda          88564\n",
      "Contra Costa     48120\n",
      "San Francisco    45344\n",
      "San Mateo        37632\n",
      "Sonoma           26276\n",
      "Solano           24900\n",
      "Marin            14040\n",
      "Napa              8754\n",
      "Name: county_name, dtype: int64\n",
      "Number of NaN values in 'county_name' after update: 31974\n",
      " - Time taken: 28.59 seconds\n",
      "########################## PATHTRAVERSAL ####################\n",
      " - Function:  filter_pathtraversal ...\n",
      "     ... Len Events 34853018\n",
      "     ... Len pathTraversal 8555906\n",
      "     ... Total Length in Meters PathTraversal 29423663488.663895\n",
      " - Time taken: 0.87 seconds\n",
      " - Function:  drop_na_links ...\n",
      "     ... Total Length in Meters PathTraversal 29423663488.663895\n",
      "     ... Len pathTraversal 8555906\n",
      "     ... Total Length in Meters PathTraversal after dropping Nan links 28340407646.612988\n",
      "     ... Len pathTraversal after dropping Nan links 7166050\n",
      " - Time taken: 1.45 seconds\n",
      " - Function:  filter_modes ...\n",
      "     ... Modes of pathTraversal Index(['car', 'walk', 'bus', 'car_hov2', 'car_hov3', 'bike'], dtype='object')\n",
      "     ... Len pathTraversal after dropping active modes 4484090\n",
      "     ... Total Length in Meters PathTraversal after dropping active modes 23582956343.513992\n",
      "     ... Total Length in Meters PathTraversal of freight vehicles 2169523861.135\n",
      "     ... Total Length in Meters PathTraversal of car vehicles 20430447366.49001\n",
      "     ... Total Length in Meters PathTraversal of bus vehicles 982985115.889\n",
      "     ... Len pathTraversal Freight 160099\n",
      "     ... Len pathTraversal Car 3050510\n",
      "     ... Len pathTraversal Bus 1273481\n",
      " - Time taken: 2.85 seconds\n",
      " - Function:  process_links ...\n",
      "     ... Total Length in Meters PathTraversal after dropping rows with None in firstLink or lastLink 23582956343.513992\n",
      "     ... Len pathTraversal after dropping rows with None in firstLink or lastLink 4484090\n",
      " - Time taken: 101.94 seconds\n",
      " - Function:  link_times ...\n",
      " - Time taken: 138.92 seconds\n",
      "########################## NETWORK ##########################\n",
      " - Function:  calculate_midpoints ...\n",
      " - Time taken: 0.01 seconds\n",
      " - Function:  enrich_geo_data ...\n",
      " - Function:  addGeometryIdToDataFrame ...\n",
      " - Time taken: 1.33 seconds\n",
      " - Function:  addGeometryIdToDataFrame ...\n",
      " - Time taken: 5.82 seconds\n",
      "Santa Clara County      78950\n",
      "Alameda County          77606\n",
      "San Francisco County    43974\n",
      "Contra Costa County     37376\n",
      "San Mateo County        31072\n",
      "Solano County           15414\n",
      "Sonoma County           13738\n",
      "Marin County             9148\n",
      "Napa County              4010\n",
      "Name: county_name, dtype: int64\n",
      " - Function:  addGeometryIdToDataFrame ...\n",
      " - Time taken: 13.64 seconds\n",
      "Santa Clara      78950\n",
      "Alameda          77592\n",
      "San Francisco    43976\n",
      "Contra Costa     37392\n",
      "San Mateo        31074\n",
      "Solano           15424\n",
      "Sonoma           13738\n",
      "Marin             9152\n",
      "Napa              3998\n",
      "Name: county_name, dtype: int64\n",
      "Number of NaN values in 'county_name' after update: 7450\n",
      " - Time taken: 21.18 seconds\n",
      "########################## PARK #############################\n",
      " - Function:  prepare_park_departDatabase ...\n",
      " - Time taken: 82.22 seconds\n",
      " - Function:  prepare_parkingDatabase ...\n",
      " - Time taken: 15.80 seconds\n",
      "########################## EXPLODE ##########################\n",
      " - Function:  explode_path_traversal ...\n",
      " - Time taken: 494.43 seconds\n",
      " - Function:  merge_with_network ...\n",
      "     ... Len explodedPathTraversal 158232669\n",
      "     ... Len explodedPathTraversal after dropping trip outside the simulated counties:  155620503\n",
      " - Time taken: 253.80 seconds\n",
      " - Function:  adjust_travel_times ...\n",
      " - Time taken: 443.58 seconds\n",
      " - Function:  calculate_speeds ...\n",
      "     ... Len explodedPathTraversal 155620503\n",
      "     ... Len of undetermined speeds: 1242551\n",
      " - Time taken: 9.16 seconds\n",
      "155620503 - len of explodedPathTraversal after processing\n",
      "########################## ASSIGN VEHICLE CLASS #############\n",
      " - Function:  load_EMFACT_data ...\n",
      "   Loaded 1,013 rows from Default_MTC_2018_Annual_vmt_20240313111517.csv\n",
      "   Unaccounted vehicle classes (49): ['All Other Buses' 'LHD1' 'LHD2' 'MCY' 'MH' 'Motor Coach' 'OBUS' 'PTO'\n",
      " 'SBUS' 'T6 CAIRP Class 4' 'T6 CAIRP Class 5' 'T6 CAIRP Class 6'\n",
      " 'T6 CAIRP Class 7' 'T6 Instate Delivery Class 4'\n",
      " 'T6 Instate Delivery Class 5' 'T6 Instate Delivery Class 6'\n",
      " 'T6 Instate Other Class 4' 'T6 Instate Other Class 5'\n",
      " 'T6 Instate Other Class 6' 'T6 Instate Other Class 7'\n",
      " 'T6 Instate Tractor Class 7' 'T6 OOS Class 4' 'T6 OOS Class 5'\n",
      " 'T6 OOS Class 6' 'T6 OOS Class 7' 'T6 Public Class 4' 'T6 Public Class 5'\n",
      " 'T6 Public Class 6' 'T6 Public Class 7' 'T6 Utility Class 5'\n",
      " 'T6 Utility Class 6' 'T6 Utility Class 7' 'T6TS' 'T7 CAIRP Class 8'\n",
      " 'T7 NNOOS Class 8' 'T7 NOOS Class 8' 'T7 Other Port Class 8'\n",
      " 'T7 POAK Class 8' 'T7 Public Class 8' 'T7 Single Dump Class 8'\n",
      " 'T7 Single Other Class 8' 'T7 SWCV Class 8' 'T7 Tractor Class 8'\n",
      " 'T7 Utility Class 8' 'T7IS' 'UBUS' 'T6 Instate Delivery Class 7'\n",
      " 'T7 Single Concrete/Transit Mix Class 8' 'T6 Instate Tractor Class 6']\n",
      "   Remaining after mode filter: 176 rows\n",
      "   After county filter: 144 rows in 9 counties\n",
      " - Time taken: 0.03 seconds\n",
      " - Function:  load_EMFACT_data ...\n",
      "   Loaded 1,013 rows from Default_MTC_2018_Annual_vmt_20240313111517.csv\n",
      "   Unaccounted vehicle classes (10): ['All Other Buses' 'LDA' 'LDT1' 'LDT2' 'MDV' 'Motor Coach' 'OBUS' 'PTO'\n",
      " 'SBUS' 'UBUS']\n",
      "   Remaining after mode filter: 724 rows\n",
      "   After county filter: 629 rows in 9 counties\n",
      " - Time taken: 0.01 seconds\n",
      " - Function:  calculate_vmt_distribution by county ...\n",
      "   Created VMT distribution for 9 counties\n",
      "   Example distribution rows:\n",
      "         sub_area vehicle_class  fuel     total_vmt  total_vmt_county  \\\n",
      "0  Alameda County           LDA   Dsl  9.216397e+04      3.673321e+07   \n",
      "1  Alameda County           LDA  Elec  7.277545e+05      3.673321e+07   \n",
      "2  Alameda County           LDA   Gas  2.076490e+07      3.673321e+07   \n",
      "3  Alameda County           LDA   Phe  3.212361e+05      3.673321e+07   \n",
      "4  Alameda County          LDT1   Dsl  1.235672e+03      3.673321e+07   \n",
      "\n",
      "   vmt_distribution  \n",
      "0          0.002509  \n",
      "1          0.019812  \n",
      "2          0.565290  \n",
      "3          0.008745  \n",
      "4          0.000034  \n",
      " - Time taken: 0.03 seconds\n",
      " - Function:  calculate_vmt_distribution by county ...\n",
      "   Created VMT distribution for 9 counties\n",
      "   Example distribution rows:\n",
      "         sub_area vehicle_class fuel      total_vmt  total_vmt_county  \\\n",
      "0  Alameda County          LHD1  Dsl  295486.374382      3.779639e+06   \n",
      "1  Alameda County          LHD1  Gas  687456.803064      3.779639e+06   \n",
      "2  Alameda County          LHD2  Dsl  110541.728758      3.779639e+06   \n",
      "3  Alameda County          LHD2  Gas   98625.040906      3.779639e+06   \n",
      "4  Alameda County           MCY  Gas  150409.900984      3.779639e+06   \n",
      "\n",
      "   vmt_distribution  \n",
      "0          0.078178  \n",
      "1          0.181884  \n",
      "2          0.029247  \n",
      "3          0.026094  \n",
      "4          0.039795  \n",
      " - Time taken: 0.01 seconds\n",
      " - Function:  create_vmtLookup ...\n",
      "   Total counties in lookup: 9\n",
      " - Time taken: 0.00 seconds\n",
      " - Function:  create_vmtLookup ...\n",
      "   Total counties in lookup: 9\n",
      " - Time taken: 0.00 seconds\n",
      " - Function:  assign_vehicle_classes ...\n",
      "len dataframe: 155620503\n",
      "Alameda County - len car dataframe: 31836804\n",
      "Alameda County - len freight dataframe: 3843912\n",
      "Alameda County - len bus dataframe: 2014444\n",
      "   Assigning 31836804 rows for county 'Alameda County' from 16 EMFACT classes\n",
      "   Assigning 3843912 rows for county 'Alameda County' from 69 EMFACT classes\n",
      "Contra Costa County - len car dataframe: 22098470\n",
      "Contra Costa County - len freight dataframe: 2159984\n",
      "Contra Costa County - len bus dataframe: 937369\n",
      "   Assigning 22098470 rows for county 'Contra Costa County' from 16 EMFACT classes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Assigning 2159984 rows for county 'Contra Costa County' from 70 EMFACT classes\n",
      "Marin County - len car dataframe: 6701860\n",
      "Marin County - len freight dataframe: 274662\n",
      "Marin County - len bus dataframe: 530897\n",
      "   Assigning 6701860 rows for county 'Marin County' from 16 EMFACT classes\n",
      "   Assigning 274662 rows for county 'Marin County' from 69 EMFACT classes\n",
      "Napa County - len car dataframe: 3301795\n",
      "Napa County - len freight dataframe: 170053\n",
      "Napa County - len bus dataframe: 68307\n",
      "   Assigning 3301795 rows for county 'Napa County' from 16 EMFACT classes\n",
      "   Assigning 170053 rows for county 'Napa County' from 69 EMFACT classes\n",
      "San Francisco County - len car dataframe: 8969118\n",
      "San Francisco County - len freight dataframe: 379194\n",
      "San Francisco County - len bus dataframe: 2471831\n",
      "   Assigning 8969118 rows for county 'San Francisco County' from 16 EMFACT classes\n",
      "   Assigning 379194 rows for county 'San Francisco County' from 71 EMFACT classes\n",
      "San Mateo County - len car dataframe: 11910903\n",
      "San Mateo County - len freight dataframe: 485971\n",
      "San Mateo County - len bus dataframe: 661819\n",
      "   Assigning 11910903 rows for county 'San Mateo County' from 16 EMFACT classes\n",
      "   Assigning 485971 rows for county 'San Mateo County' from 71 EMFACT classes\n",
      "Santa Clara County - len car dataframe: 31250674\n",
      "Santa Clara County - len freight dataframe: 2712425\n",
      "Santa Clara County - len bus dataframe: 1721202\n",
      "   Assigning 31250674 rows for county 'Santa Clara County' from 16 EMFACT classes\n",
      "   Assigning 2712425 rows for county 'Santa Clara County' from 70 EMFACT classes\n",
      "Solano County - len car dataframe: 8587768\n",
      "Solano County - len freight dataframe: 1335795\n",
      "Solano County - len bus dataframe: 212510\n",
      "   Assigning 8587768 rows for county 'Solano County' from 16 EMFACT classes\n",
      "   Assigning 1335795 rows for county 'Solano County' from 70 EMFACT classes\n",
      "Sonoma County - len car dataframe: 10292086\n",
      "Sonoma County - len freight dataframe: 439133\n",
      "Sonoma County - len bus dataframe: 251517\n",
      "   Assigning 10292086 rows for county 'Sonoma County' from 16 EMFACT classes\n",
      "   Assigning 439133 rows for county 'Sonoma County' from 70 EMFACT classes\n",
      " - Time taken: 753.05 seconds\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'stop_here' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1230>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1311\u001b[0m freight_vmtLookup \u001b[38;5;241m=\u001b[39m create_vmtLookup(freight_totalVMTbyCounty)\n\u001b[1;32m   1313\u001b[0m explodedPathTraversal \u001b[38;5;241m=\u001b[39m assign_vehicle_classes(explodedPathTraversal, car_vmtLookup, freight_vmtLookup)\n\u001b[0;32m-> 1314\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mstop_here\u001b[49m)\n\u001b[1;32m   1315\u001b[0m parkingDatabase \u001b[38;5;241m=\u001b[39m assign_vehicle_classes(parkingDatabase, car_vmtLookup, freight_vmtLookup)\n\u001b[1;32m   1316\u001b[0m departDatabase \u001b[38;5;241m=\u001b[39m assign_vehicle_classes(departDatabase, car_vmtLookup, freight_vmtLookup)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop_here' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "import bisect\n",
    "import time\n",
    "import gc\n",
    "\n",
    " \n",
    "\n",
    "#############################################################\n",
    "########################## INPUTS ###########################\n",
    "#############################################################\n",
    "\n",
    "# other_county = 'San Francisco County' # When a link falls in the water or outside county boundaries, like on bridges, consider it under this EMFAC county in order not to be excluded for emisison calculation\n",
    "                                      # However, this is replaced with Oter for plots on isrmToCountyDict\n",
    "\n",
    "# corr_VMT_by_county_dict = {('Alameda County', 'LDA'): 0.8382584178761276, ('Alameda County', 'LDT1'): 0.8382955189879107, ('Alameda County', 'LDT2'): 0.8384195964699777, ('Alameda County', 'MDV'): 0.8384078994409043, ('Alameda County', 'UBUS'): 1.0, ('Contra Costa County', 'LDA'): 1.0364590034773495, ('Contra Costa County', 'LDT1'): 1.0354872989789385, ('Contra Costa County', 'LDT2'): 1.036609238618241, ('Contra Costa County', 'MDV'): 1.036809838645456, ('Contra Costa County', 'UBUS'): 1.0, ('Marin County', 'LDA'): 0.8350251329301085, ('Marin County', 'LDT1'): 0.8364646611758678, ('Marin County', 'LDT2'): 0.8354799512452032, ('Marin County', 'MDV'): 0.8354492129920658, ('Marin County', 'UBUS'): 1.0, ('Napa County', 'LDA'): 0.8461647920705302, ('Napa County', 'LDT1'): 0.8458556105737716, ('Napa County', 'LDT2'): 0.8464635131944629, ('Napa County', 'MDV'): 0.8467133218850701, ('Napa County', 'UBUS'): 1.0, ('San Francisco County', 'LDA'): 0.9632673134665494, ('San Francisco County', 'LDT1'): 0.9611247721072494, ('San Francisco County', 'LDT2'): 0.9635388441890713, ('San Francisco County', 'MDV'): 0.963717170418973, ('San Francisco County', 'UBUS'): 1.0, ('San Mateo County', 'LDA'): 0.8665326830803317, ('San Mateo County', 'LDT1'): 0.8668840511668878, ('San Mateo County', 'LDT2'): 0.866152509448014, ('San Mateo County', 'MDV'): 0.8671667712889382, ('San Mateo County', 'UBUS'): 1.0, ('Santa Clara County', 'LDA'): 0.8280820774305037, ('Santa Clara County', 'LDT1'): 0.8278832876939534, ('Santa Clara County', 'LDT2'): 0.8278219637335007, ('Santa Clara County', 'MDV'): 0.8284479156732447, ('Santa Clara County', 'UBUS'): 1.0, ('Solano County', 'LDA'): 1.295980878198694, ('Solano County', 'LDT1'): 1.294765565973038, ('Solano County', 'LDT2'): 1.295497537235339, ('Solano County', 'MDV'): 1.2963054635632012, ('Solano County', 'UBUS'): 1.0, ('Sonoma County', 'LDA'): 1.4011986791374236, ('Sonoma County', 'LDT1'): 1.4010809579562558, ('Sonoma County', 'LDT2'): 1.4010050148776163, ('Sonoma County', 'MDV'): 1.4019799515123095, ('Sonoma County', 'UBUS'): 1.0}\n",
    "# corr_Trips_by_county_dict = {('Alameda County', 'LDA'): 0.7047680690771602, ('Alameda County', 'LDT1'): 0.689314393083685, ('Alameda County', 'LDT2'): 0.7267671182103814, ('Alameda County', 'MDV'): 0.6907161567740274, ('Alameda County', 'UBUS'): 1.0, ('Contra Costa County', 'LDA'): 0.802898225985577, ('Contra Costa County', 'LDT1'): 0.7919667293056307, ('Contra Costa County', 'LDT2'): 0.843262307240611, ('Contra Costa County', 'MDV'): 0.7991784461558805, ('Contra Costa County', 'UBUS'): 1.0, ('Marin County', 'LDA'): 0.5510642938592881, ('Marin County', 'LDT1'): 0.5328783005592662, ('Marin County', 'LDT2'): 0.5743851117136962, ('Marin County', 'MDV'): 0.5523760316395268, ('Marin County', 'UBUS'): 1.0, ('Napa County', 'LDA'): 0.626636904959386, ('Napa County', 'LDT1'): 0.5632007875904346, ('Napa County', 'LDT2'): 0.6022609869579018, ('Napa County', 'MDV'): 0.5708766475232429, ('Napa County', 'UBUS'): 1.0, ('San Francisco County', 'LDA'): 1.2883514823468911, ('San Francisco County', 'LDT1'): 1.168491928257591, ('San Francisco County', 'LDT2'): 1.2189083712998492, ('San Francisco County', 'MDV'): 1.2984693895455655, ('San Francisco County', 'UBUS'): 1.0, ('San Mateo County', 'LDA'): 0.7523217307547722, ('San Mateo County', 'LDT1'): 0.7008868316076761, ('San Mateo County', 'LDT2'): 0.7411828603484104, ('San Mateo County', 'MDV'): 0.7427945003041081, ('San Mateo County', 'UBUS'): 1.0, ('Santa Clara County', 'LDA'): 0.9091343513181342, ('Santa Clara County', 'LDT1'): 0.839523566483252, ('Santa Clara County', 'LDT2'): 0.8689378108899927, ('Santa Clara County', 'MDV'): 0.8388883504691426, ('Santa Clara County', 'UBUS'): 1.0, ('Solano County', 'LDA'): 1.1853792192875006, ('Solano County', 'LDT1'): 1.0342434543771066, ('Solano County', 'LDT2'): 1.1050388141925946, ('Solano County', 'MDV'): 1.0675685971050919, ('Solano County', 'UBUS'): 1.0, ('Sonoma County', 'LDA'): 1.0796644236379482, ('Sonoma County', 'LDT1'): 0.9756880678880802, ('Sonoma County', 'LDT2'): 1.0448094446183394, ('Sonoma County', 'MDV'): 1.0265947791976873, ('Sonoma County', 'UBUS'): 1.0}\n",
    "\n",
    "cruise_class_type = 'LDA'\n",
    "cruise_fuel_type = 'Elec'\n",
    "\n",
    "scenarios = [\n",
    "#     'sfbay-baseline3_20240728',\n",
    "#     'sfbay-cordon_flatrate_20241023',\n",
    "#     'sfbay-cordon_income_20241023',\n",
    "    \n",
    "#             'sfbay-tr_capacity_1_5-20230608',\n",
    "#             'sfbay-wb-incentives-200-20230630',\n",
    "#             'sfbay-tr-discount-100-20230703',\n",
    "             \n",
    "#             'sfbay-telecommuting-baseline-20230616',\n",
    "#             'sfbay-telecommuting-8p60-20230620',\n",
    "             \n",
    "#             'sfbay_cruise_SAVBaseline_1_phase2_97',\n",
    "#             'sfbay_cruise_phase2_a1b3c3d1',\n",
    "#             'sfbay_cruise_phase2_a1b4c5d1',\n",
    "#             'sfbay_cruise_phase2_a1b4c3d1',\n",
    "             \n",
    "#             'sfbay-baseline2018-30pct-20230825',\n",
    "#             'sfbay-tr-30pct-20231014'\n",
    "    \n",
    "#             'sfbay-emissions--20240123--2018-Baseline__2025-04-10_03-11-41_sjh',\n",
    "            'sfbay-emissions--20240123--2018-Baseline__2025-04-13_21-05-01_iek'\n",
    "    \n",
    "            ]\n",
    "\n",
    "\n",
    "#Scenarios must be in order: when is baseline, calculate \n",
    "# the correction factors to be used for the next scenarios, \n",
    "# until there is a next baseline\n",
    "are_baseline = [\n",
    "#                 True,\n",
    "#                 False,\n",
    "#                 False,\n",
    "    \n",
    "#                 True,\n",
    "#                 False,\n",
    "#                 False,\n",
    "\n",
    "#                 True,\n",
    "#                 False,\n",
    "\n",
    "#                 True,\n",
    "#                 False,\n",
    "#                 False,\n",
    "#                 False,\n",
    "    \n",
    "#                 True,\n",
    "#                 False,\n",
    "    \n",
    "#     True,\n",
    "    True\n",
    "               ]\n",
    "years = [\n",
    "#     '2020',\n",
    "#     '2020',\n",
    "#     '2020',\n",
    "    \n",
    "#         '2020',\n",
    "#         '2020',\n",
    "#         '2020',\n",
    "         \n",
    "#         '2020',\n",
    "#         '2020',\n",
    "         \n",
    "#         '2020',\n",
    "#         '2020',\n",
    "#         '2020',\n",
    "#         '2020',\n",
    "        \n",
    "#         '2018',\n",
    "#         '2018'\n",
    "    \n",
    "#     '',\n",
    "    ''\n",
    "                ]\n",
    "iterations = [\n",
    "#     '3',\n",
    "#     '3',\n",
    "#     '3',\n",
    "    \n",
    "#             '4',\n",
    "#              '4',\n",
    "#              '4',\n",
    "              \n",
    "#              '4',\n",
    "#              '4',\n",
    "              \n",
    "#              '3',\n",
    "#              '3',\n",
    "#              '3',\n",
    "#              '3',\n",
    "                \n",
    "#             '10',\n",
    "#             '10',\n",
    "    \n",
    "#     '',\n",
    "    ''\n",
    "                    ]\n",
    "\n",
    "scales = [\n",
    "#     0.1,\n",
    "#     0.1,\n",
    "#     0.1,\n",
    "    \n",
    "#             0.1,\n",
    "#          0.1,\n",
    "#          0.1,\n",
    "          \n",
    "#          0.1,\n",
    "#          0.1,\n",
    "          \n",
    "#          0.1,\n",
    "#          0.1,\n",
    "#          0.1,\n",
    "#          0.1,\n",
    "         \n",
    "#          0.3,\n",
    "#          0.3,\n",
    "    \n",
    "#     0.1,\n",
    "    0.1\n",
    "            ]\n",
    "\n",
    "buckets = [\n",
    "#             'beam-core-outputs',\n",
    "#             'beam-core-outputs',\n",
    "#             'beam-core-outputs',\n",
    "    \n",
    "#             'beam-core-outputs',\n",
    "#           'beam-core-outputs',\n",
    "#           'beam-core-outputs',\n",
    "           \n",
    "#           'beam-core-outputs',\n",
    "#           'beam-core-outputs',\n",
    "           \n",
    "#           'cruise-outputs',\n",
    "#           'cruise-outputs',\n",
    "#           'cruise-outputs',\n",
    "#           'cruise-outputs',\n",
    "           \n",
    "#           'beam-core-outputs',\n",
    "#           'beam-core-outputs',\n",
    "    \n",
    "#     'beam-core-outputs/output/testing/job_67f72d0565516f2aaa14d443'\n",
    "    'beam-core-outputs/output/testing/job_67fc238a350864f86e0f2c0b'\n",
    "          ]\n",
    "\n",
    "are_cruise = [\n",
    "#     False, \n",
    "#     False, \n",
    "#     False, \n",
    "    \n",
    "#             False,\n",
    "#               False,\n",
    "#               False,\n",
    "              \n",
    "#               False,\n",
    "#               False,\n",
    "              \n",
    "#               True,\n",
    "#               True,\n",
    "#               True,\n",
    "#               True, \n",
    "            \n",
    "#               False,\n",
    "#               False\n",
    "    \n",
    "#     False,\n",
    "    False\n",
    "]\n",
    "\n",
    "is_HaitamTest = [\n",
    "#     False, \n",
    "#     False, \n",
    "#     False, \n",
    "    \n",
    "#             False,\n",
    "#               False,\n",
    "#               False,\n",
    "              \n",
    "#               False,\n",
    "#               False,\n",
    "              \n",
    "#             False,\n",
    "#               False,\n",
    "#               False,\n",
    "#               False,\n",
    "            \n",
    "#               False,\n",
    "#               False\n",
    "    \n",
    "#     True,\n",
    "    True\n",
    "]\n",
    "\n",
    "is_baseline = True # Recalculate correction factors\n",
    "\n",
    "is_cruise = False # Correct Cruise vehicle types\n",
    "\n",
    "# scenario = 'sfbay-baseline-20230526'\n",
    "# scenario = 'sfbay-tr_capacity_1_5-20230608'\n",
    "# scenario = 'sfbay-cordon-MTA-20230703'\n",
    "# scenario = 'sfbay-wb-incentives-200-20230630'\n",
    "\n",
    "# scenario = 'sfbay-baseline2018-30pct-20230825'\n",
    "# scenario = 'sfbay-tr-30pct-20231014'\n",
    "\n",
    "# scenario = 'sfbay-telecommuting-baseline-20230616'\n",
    "# scenario = 'sfbay-telecommuting-8p60-20230620'\n",
    "\n",
    "# scenario = 'sfbay_cruise_SAVBaseline_1_phase2_97'\n",
    "# scenario = 'sfbay_cruise_phase2_a1b3c3d1'\n",
    "# scenario = 'sfbay_cruise_phase2_a1b4c5d1'\n",
    "# scenario = 'sfbay_cruise_phase2_a1b4c3d1'\n",
    "\n",
    "\n",
    "# year = '2018'\n",
    "# iteration = '10'\n",
    "# scale = 0.3\n",
    "\n",
    "# year = '2020'\n",
    "# iteration = '4'\n",
    "# scale = 0.1\n",
    "# bucket = 'beam-core-outputs'\n",
    "\n",
    "\n",
    "\n",
    "nrows_events = None\n",
    "\n",
    "geoAggregationType = 'ISRM' \n",
    "correction_factor_curvature = 1.0\n",
    "block_info = pd.read_csv('/Users/cpoliziani/Documents/repo/beam-core-analysis/Users/Nazanin/JoeFish_BlockGroup_Labels/bg_w_geog_labels.csv',\n",
    "                         dtype={'bgid': str, 'tractid': str})\n",
    "block_info['bgid'] = '0' + block_info['bgid'].astype(str) \n",
    "BGs = gpd.read_file('/Users/cpoliziani/Documents/repo/beam-core-analysis/Users/Nazanin/Shapefile2010/641aa0d4-ce5b-4a81-9c30-8790c4ab8cfb202047-1-wkkklf.j5ouj.shp')\n",
    "ISRM_grid = gpd.read_file('isrm_polygon/isrm_polygon.shp')\n",
    "counties_shapefile = gpd.read_file('/Users/cpoliziani/Downloads/region_county_5910830457166027147/region_county.shp')\n",
    "\n",
    "EMFACT_VMT_filepath = 'Default_MTC_2018_Annual_vmt_20240313111517.csv'\n",
    "EMFACT_trips_filepath = 'Default_MTC_2018_Annual_trips_20240313111517.csv'\n",
    "EMFACTemfactFilepath = 'imputed_MTC_emission_rate_agg_NH3_added.csv'\n",
    "EMFACTemissionsFilepath = 'Default_MTC_2018_Annual_emission_20240313111517.csv'\n",
    "segment_length = 300  # meters\n",
    " \n",
    "days = 330\n",
    "default_speed = 40 #mph when not specified\n",
    "parking_limit = 60\n",
    "end_day_parking = 29*3600\n",
    "beginning_day_parking = (5*3600)\n",
    "seed = 105\n",
    "\n",
    "correction_factor_duration = 1.0\n",
    "   \n",
    "\n",
    "# #Baseline\n",
    "# corr_VMT_by_county_dict =   {('Alameda County', 'LDA'): 0.8309246072433601, ('Alameda County', 'LDT1'): 0.8310758447862574, ('Alameda County', 'LDT2'): 0.8313088623777907, ('Alameda County', 'MDV'): 0.8310083342522308, ('Alameda County', 'UBUS'): 1.0, ('Contra Costa County', 'LDA'): 1.033735832908966, ('Contra Costa County', 'LDT1'): 1.0338351289963696, ('Contra Costa County', 'LDT2'): 1.0333044923754449, ('Contra Costa County', 'MDV'): 1.0343219194927749, ('Contra Costa County', 'UBUS'): 1.0, ('Marin County', 'LDA'): 0.8592816168500973, ('Marin County', 'LDT1'): 0.8600807360838104, ('Marin County', 'LDT2'): 0.8604278805799475, ('Marin County', 'MDV'): 0.8601664274778759, ('Marin County', 'UBUS'): 1.0, ('Napa County', 'LDA'): 0.8482688339393221, ('Napa County', 'LDT1'): 0.8488538654672606, ('Napa County', 'LDT2'): 0.8474093148764735, ('Napa County', 'MDV'): 0.8482049183258689, ('Napa County', 'UBUS'): 1.0, ('San Francisco County', 'LDA'): 0.9549358551340453, ('San Francisco County', 'LDT1'): 0.9551103178545439, ('San Francisco County', 'LDT2'): 0.9544763590079611, ('San Francisco County', 'MDV'): 0.9539689509490054, ('San Francisco County', 'UBUS'): 1.0, ('San Mateo County', 'LDA'): 0.8747048647328706, ('San Mateo County', 'LDT1'): 0.8745750344012946, ('San Mateo County', 'LDT2'): 0.8748760656998817, ('San Mateo County', 'MDV'): 0.874774762534271, ('San Mateo County', 'UBUS'): 1.0, ('Santa Clara County', 'LDA'): 0.8295052204636604, ('Santa Clara County', 'LDT1'): 0.829417216568818, ('Santa Clara County', 'LDT2'): 0.8294559098635733, ('Santa Clara County', 'MDV'): 0.829646215560378, ('Santa Clara County', 'UBUS'): 1.0, ('Solano County', 'LDA'): 1.3250414194391953, ('Solano County', 'LDT1'): 1.3263127547052709, ('Solano County', 'LDT2'): 1.3247367057668684, ('Solano County', 'MDV'): 1.3258652425693782, ('Solano County', 'UBUS'): 1.0, ('Sonoma County', 'LDA'): 1.4237514900884463, ('Sonoma County', 'LDT1'): 1.4230662335837894, ('Sonoma County', 'LDT2'): 1.4241932671740207, ('Sonoma County', 'MDV'): 1.424221619330168, ('Sonoma County', 'UBUS'): 1.0}\n",
    "# corr_Trips_by_county_dict = {('Alameda County', 'LDA'): 0.7091597506569293, ('Alameda County', 'LDT1'): 0.6936323597508934, ('Alameda County', 'LDT2'): 0.7312968301659862, ('Alameda County', 'MDV'): 0.695013598925902, ('Alameda County', 'UBUS'): 1.0, ('Contra Costa County', 'LDA'): 0.8122153579138889, ('Contra Costa County', 'LDT1'): 0.8011148598421644, ('Contra Costa County', 'LDT2'): 0.8530516728370202, ('Contra Costa County', 'MDV'): 0.8084326383470113, ('Contra Costa County', 'UBUS'): 1.0, ('Marin County', 'LDA'): 0.5577073646904817, ('Marin County', 'LDT1'): 0.5391600746601661, ('Marin County', 'LDT2'): 0.5813144784963997, ('Marin County', 'MDV'): 0.5590469075196355, ('Marin County', 'UBUS'): 1.0, ('Napa County', 'LDA'): 0.6357106636630795, ('Napa County', 'LDT1'): 0.5711720562226346, ('Napa County', 'LDT2'): 0.6111369616248149, ('Napa County', 'MDV'): 0.5790350554407057, ('Napa County', 'UBUS'): 1.0, ('San Francisco County', 'LDA'): 1.2904878233981214, ('San Francisco County', 'LDT1'): 1.1706782047854523, ('San Francisco County', 'LDT2'): 1.2209977032428923, ('San Francisco County', 'MDV'): 1.300734984990792, ('San Francisco County', 'UBUS'): 1.0, ('San Mateo County', 'LDA'): 0.7566152007867873, ('San Mateo County', 'LDT1'): 0.7049478035053239, ('San Mateo County', 'LDT2'): 0.7454133561266547, ('San Mateo County', 'MDV'): 0.7470495206515292, ('San Mateo County', 'UBUS'): 1.0, ('Santa Clara County', 'LDA'): 0.9147138591092338, ('Santa Clara County', 'LDT1'): 0.8446889699427009, ('Santa Clara County', 'LDT2'): 0.8742638501011555, ('Santa Clara County', 'MDV'): 0.8440384932282214, ('Santa Clara County', 'UBUS'): 1.0, ('Solano County', 'LDA'): 1.2027290804944388, ('Solano County', 'LDT1'): 1.049380804461785, ('Solano County', 'LDT2'): 1.121255444037017, ('Solano County', 'MDV'): 1.0832065938854636, ('Solano County', 'UBUS'): 1.0, ('Sonoma County', 'LDA'): 1.0979105108539893, ('Sonoma County', 'LDT1'): 0.9921343975310387, ('Sonoma County', 'LDT2'): 1.0624719218248007, ('Sonoma County', 'MDV'): 1.0438817373596945, ('Sonoma County', 'UBUS'): 1.0}\n",
    "# #Baseline Telecommuting\n",
    "# corr_VMT_by_county_dict =   {('Alameda County', 'LDA'): 0.785363107733696, ('Alameda County', 'LDT1'): 0.7853564165979886, ('Alameda County', 'LDT2'): 0.785390797817228, ('Alameda County', 'MDV'): 0.7849956777912425, ('Alameda County', 'UBUS'): 1.0, ('Contra Costa County', 'LDA'): 0.9872432530089924, ('Contra Costa County', 'LDT1'): 0.987048936093546, ('Contra Costa County', 'LDT2'): 0.9876268814954524, ('Contra Costa County', 'MDV'): 0.9872268057399457, ('Contra Costa County', 'UBUS'): 1.0, ('Marin County', 'LDA'): 0.8172879096205063, ('Marin County', 'LDT1'): 0.8185103988462367, ('Marin County', 'LDT2'): 0.8174120228282623, ('Marin County', 'MDV'): 0.8182681812704706, ('Marin County', 'UBUS'): 1.0, ('Napa County', 'LDA'): 0.8458142553951628, ('Napa County', 'LDT1'): 0.8467611067417001, ('Napa County', 'LDT2'): 0.8457382327681194, ('Napa County', 'MDV'): 0.8457574922686403, ('Napa County', 'UBUS'): 1.0, ('San Francisco County', 'LDA'): 0.8993073501587944, ('San Francisco County', 'LDT1'): 0.9005953463714442, ('San Francisco County', 'LDT2'): 0.899192784589715, ('San Francisco County', 'MDV'): 0.8992279905520532, ('San Francisco County', 'UBUS'): 1.0, ('San Mateo County', 'LDA'): 0.8124041598123923, ('San Mateo County', 'LDT1'): 0.8109831615917596, ('San Mateo County', 'LDT2'): 0.812035815114161, ('San Mateo County', 'MDV'): 0.8128831432733896, ('San Mateo County', 'UBUS'): 1.0, ('Santa Clara County', 'LDA'): 0.7857616435243143, ('Santa Clara County', 'LDT1'): 0.7857972813436149, ('Santa Clara County', 'LDT2'): 0.7858681090323005, ('Santa Clara County', 'MDV'): 0.7858562582893761, ('Santa Clara County', 'UBUS'): 1.0, ('Solano County', 'LDA'): 1.2742151495807013, ('Solano County', 'LDT1'): 1.2733758023012685, ('Solano County', 'LDT2'): 1.2747147253340616, ('Solano County', 'MDV'): 1.2743458197808435, ('Solano County', 'UBUS'): 1.0, ('Sonoma County', 'LDA'): 1.3656619432175308, ('Sonoma County', 'LDT1'): 1.3656938407153925, ('Sonoma County', 'LDT2'): 1.3668753119486865, ('Sonoma County', 'MDV'): 1.366124572683926, ('Sonoma County', 'UBUS'): 1.0}\n",
    "# corr_Trips_by_county_dict = {('Alameda County', 'LDA'): 0.6826851725666049, ('Alameda County', 'LDT1'): 0.6677245597476429, ('Alameda County', 'LDT2'): 0.7039856109116475, ('Alameda County', 'MDV'): 0.6690661641149621, ('Alameda County', 'UBUS'): 1.0, ('Contra Costa County', 'LDA'): 0.7670701948598858, ('Contra Costa County', 'LDT1'): 0.756624262263867, ('Contra Costa County', 'LDT2'): 0.8056481451499258, ('Contra Costa County', 'MDV'): 0.7635208244086293, ('Contra Costa County', 'UBUS'): 1.0, ('Marin County', 'LDA'): 0.5392965683868883, ('Marin County', 'LDT1'): 0.5213872503746932, ('Marin County', 'LDT2'): 0.5620706370312918, ('Marin County', 'MDV'): 0.5406848058702638, ('Marin County', 'UBUS'): 1.0, ('Napa County', 'LDA'): 0.6207311395962061, ('Napa County', 'LDT1'): 0.5576822169989116, ('Napa County', 'LDT2'): 0.5966917479512112, ('Napa County', 'MDV'): 0.5656849333939483, ('Napa County', 'UBUS'): 1.0, ('San Francisco County', 'LDA'): 1.2424132583226901, ('San Francisco County', 'LDT1'): 1.1268069224597026, ('San Francisco County', 'LDT2'): 1.1755842994997672, ('San Francisco County', 'MDV'): 1.2522134825388531, ('San Francisco County', 'UBUS'): 1.0, ('San Mateo County', 'LDA'): 0.7229052386151321, ('San Mateo County', 'LDT1'): 0.6735675661144088, ('San Mateo County', 'LDT2'): 0.7121928313838096, ('San Mateo County', 'MDV'): 0.7137063008601003, ('San Mateo County', 'UBUS'): 1.0, ('Santa Clara County', 'LDA'): 0.8734394028335127, ('Santa Clara County', 'LDT1'): 0.8065304612880395, ('Santa Clara County', 'LDT2'): 0.834829214293623, ('Santa Clara County', 'MDV'): 0.8059331433404102, ('Santa Clara County', 'UBUS'): 1.0, ('Solano County', 'LDA'): 1.1430680793970225, ('Solano County', 'LDT1'): 0.9975726907916888, ('Solano County', 'LDT2'): 1.0656208637467885, ('Solano County', 'MDV'): 1.0294647457951722, ('Solano County', 'UBUS'): 1.0, ('Sonoma County', 'LDA'): 1.0468909755527545, ('Sonoma County', 'LDT1'): 0.945855190861318, ('Sonoma County', 'LDT2'): 1.0131414788781201, ('Sonoma County', 'MDV'): 0.9952778260056453, ('Sonoma County', 'UBUS'): 1.0}\n",
    "# #Baseline Transit Rich\n",
    "\n",
    "# #Baseline Cruise\n",
    "# corr_VMT_by_county_dict = {('Alameda County', 'LDA'): 0.7470757280963015, ('Alameda County', 'LDT1'): 0.7472140594014864, ('Alameda County', 'LDT2'): 0.7472380715194904, ('Alameda County', 'MDV'): 0.7471262895070857, ('Alameda County', 'UBUS'): 1.0, ('Contra Costa County', 'LDA'): 0.9847653531214324, ('Contra Costa County', 'LDT1'): 0.984509971601675, ('Contra Costa County', 'LDT2'): 0.9845308289181104, ('Contra Costa County', 'MDV'): 0.9847239637405645, ('Contra Costa County', 'UBUS'): 1.0, ('Marin County', 'LDA'): 0.7213519223772172, ('Marin County', 'LDT1'): 0.7212999867229127, ('Marin County', 'LDT2'): 0.7213014923777741, ('Marin County', 'MDV'): 0.721263316901008, ('Marin County', 'UBUS'): 1.0, ('Napa County', 'LDA'): 0.8131699350289362, ('Napa County', 'LDT1'): 0.8127364757621987, ('Napa County', 'LDT2'): 0.8124096211841729, ('Napa County', 'MDV'): 0.8129467981345975, ('Napa County', 'UBUS'): 1.0, ('San Francisco County', 'LDA'): 0.690365277153231, ('San Francisco County', 'LDT1'): 0.6911573784822581, ('San Francisco County', 'LDT2'): 0.690634537403338, ('San Francisco County', 'MDV'): 0.6898464226378359, ('San Francisco County', 'UBUS'): 1.0, ('San Mateo County', 'LDA'): 0.755619339380792, ('San Mateo County', 'LDT1'): 0.7565534468039801, ('San Mateo County', 'LDT2'): 0.7556177981134615, ('San Mateo County', 'MDV'): 0.7562688858176352, ('San Mateo County', 'UBUS'): 1.0, ('Santa Clara County', 'LDA'): 0.7550948471743995, ('Santa Clara County', 'LDT1'): 0.7549179970888578, ('Santa Clara County', 'LDT2'): 0.7549617843328084, ('Santa Clara County', 'MDV'): 0.7548916110007055, ('Santa Clara County', 'UBUS'): 1.0, ('Solano County', 'LDA'): 1.3281306684274687, ('Solano County', 'LDT1'): 1.3264322018480532, ('Solano County', 'LDT2'): 1.3286481555145708, ('Solano County', 'MDV'): 1.327944123036664, ('Solano County', 'UBUS'): 1.0, ('Sonoma County', 'LDA'): 1.3192556630301941, ('Sonoma County', 'LDT1'): 1.319402335782789, ('Sonoma County', 'LDT2'): 1.3199072435124304, ('Sonoma County', 'MDV'): 1.3190121792896632, ('Sonoma County', 'UBUS'): 1.0}\n",
    "# corr_Trips_by_county_dict = {('Alameda County', 'LDA'): 0.5930602881693966, ('Alameda County', 'LDT1'): 0.5800952362072366, ('Alameda County', 'LDT2'): 0.6115585952640538, ('Alameda County', 'MDV'): 0.5812290510410403, ('Alameda County', 'UBUS'): 1.0, ('Contra Costa County', 'LDA'): 0.7379894497503067, ('Contra Costa County', 'LDT1'): 0.7279298155498949, ('Contra Costa County', 'LDT2'): 0.7751301383370086, ('Contra Costa County', 'MDV'): 0.7345590004670559, ('Contra Costa County', 'UBUS'): 1.0, ('Marin County', 'LDA'): 0.5059431763954676, ('Marin County', 'LDT1'): 0.48921230985788877, ('Marin County', 'LDT2'): 0.5274633994993898, ('Marin County', 'MDV'): 0.5070553388045606, ('Marin County', 'UBUS'): 1.0, ('Napa County', 'LDA'): 0.6084893875519989, ('Napa County', 'LDT1'): 0.5463384885607809, ('Napa County', 'LDT2'): 0.584770095702514, ('Napa County', 'MDV'): 0.5543479249891623, ('Napa County', 'UBUS'): 1.0, ('San Francisco County', 'LDA'): 0.8405743980089847, ('San Francisco County', 'LDT1'): 0.7624275011494895, ('San Francisco County', 'LDT2'): 0.7954441499381745, ('San Francisco County', 'MDV'): 0.8473326965147943, ('San Francisco County', 'UBUS'): 1.0, ('San Mateo County', 'LDA'): 0.639439247829707, ('San Mateo County', 'LDT1'): 0.5958553311639659, ('San Mateo County', 'LDT2'): 0.6299430871478379, ('San Mateo County', 'MDV'): 0.631283622233763, ('San Mateo County', 'UBUS'): 1.0, ('Santa Clara County', 'LDA'): 0.7968017355126182, ('Santa Clara County', 'LDT1'): 0.7358153604065697, ('Santa Clara County', 'LDT2'): 0.7615898446656352, ('Santa Clara County', 'MDV'): 0.7352292444096753, ('Santa Clara County', 'UBUS'): 1.0, ('Solano County', 'LDA'): 1.0953976674750743, ('Solano County', 'LDT1'): 0.9564246828150279, ('Solano County', 'LDT2'): 1.0212832615504275, ('Solano County', 'MDV'): 0.9865887358433549, ('Solano County', 'UBUS'): 1.0, ('Sonoma County', 'LDA'): 0.9987039280764861, ('Sonoma County', 'LDT1'): 0.9023807845958227, ('Sonoma County', 'LDT2'): 0.9664565170989271, ('Sonoma County', 'MDV'): 0.9495801279078178, ('Sonoma County', 'UBUS'): 1.0}\n",
    "\n",
    "processes = ['RUNEX', 'PMBW', 'PMTW', 'RUNLOSS', 'HOTSOAK', 'DIURN', 'STREX']\n",
    "\n",
    "processDataframes = ['PT', 'PT', 'PT', 'PT', 'PAn', 'PA', 'DP']\n",
    "\n",
    "pollutants_list = [\n",
    "    ['PM2_5', 'SOx', 'NOx', 'ROG', 'NH3', 'CO2'],  # RUNEX\n",
    "    ['PM2_5'],                              # PMBW\n",
    "    ['PM2_5'],                              # PMTW\n",
    "    ['ROG'],                                # RUNLOSS\n",
    "    ['ROG'],                                # HOTSOAK\n",
    "    ['ROG'],                                # DIURN\n",
    "    ['PM2_5', 'SOx', 'NOx', 'ROG', 'CO2']          # STREX\n",
    "]\n",
    "\n",
    "merging_lists = [\n",
    "    ['fuel_type_emfact', 'average_speed_mph', 'county_name', 'vehicle_class_emfact'],  # RUNEX\n",
    "    ['fuel_type_emfact', 'average_speed_mph', 'county_name', 'vehicle_class_emfact'],  # PMBW\n",
    "    ['fuel_type_emfact', 'county_name', 'vehicle_class_emfact'],                       # PMTW\n",
    "    ['fuel_type_emfact', 'county_name', 'vehicle_class_emfact'],                       # RUNLOSS\n",
    "    ['fuel_type_emfact', 'county_name', 'vehicle_class_emfact'],                       # HOTSOAK\n",
    "    ['fuel_type_emfact', 'county_name', 'vehicle_class_emfact'],                       # DIURN\n",
    "    ['fuel_type_emfact', 'average_speed_mph', 'county_name', 'vehicle_class_emfact']   # STREX\n",
    "]\n",
    "\n",
    "busesFuelType = 'Dsl'\n",
    "busVehicleClassDict = {'UBUS':'UBUS',\n",
    "                       'OBUS':'UBUS',\n",
    "#                        'SBUS':'UBUS',\n",
    "#                        'All Other Buses':'UBUS',\n",
    "                      }\n",
    "pathTraversalModesDict = {'car':'car', \n",
    "                               'car_hov2':'car', \n",
    "                               'car_hov3':'car', \n",
    "                               'bus':'UBUS',\n",
    "                               'freight':'freight',\n",
    "                         }\n",
    "                               \n",
    "EMFACT_classes_car = ['LDA','LDT1', 'LDT2', 'MDV']\n",
    "EMFACT_classes_freight = [\n",
    "    'LHD1', 'LHD2', 'MCY', 'MH',\n",
    "    'T6 CAIRP Class 4', 'T6 CAIRP Class 5', 'T6 CAIRP Class 6', 'T6 CAIRP Class 7',\n",
    "    'T6 Instate Delivery Class 4', 'T6 Instate Delivery Class 5', 'T6 Instate Delivery Class 6', 'T6 Instate Delivery Class 7',\n",
    "    'T6 Instate Other Class 4', 'T6 Instate Other Class 5', 'T6 Instate Other Class 6', 'T6 Instate Other Class 7',\n",
    "    'T6 Instate Tractor Class 6', 'T6 Instate Tractor Class 7',\n",
    "    'T6 OOS Class 4', 'T6 OOS Class 5', 'T6 OOS Class 6', 'T6 OOS Class 7',\n",
    "    'T6 Public Class 4', 'T6 Public Class 5', 'T6 Public Class 6', 'T6 Public Class 7',\n",
    "    'T6 Utility Class 5', 'T6 Utility Class 6', 'T6 Utility Class 7',\n",
    "    'T6TS',\n",
    "    'T7 CAIRP Class 8', 'T7 NNOOS Class 8', 'T7 NOOS Class 8', 'T7 Other Port Class 8', 'T7 POAK Class 8',\n",
    "    'T7 Public Class 8',\n",
    "    'T7 Single Concrete/Transit Mix Class 8', 'T7 Single Dump Class 8', 'T7 Single Other Class 8',\n",
    "    'T7 SWCV Class 8', 'T7 Tractor Class 8', 'T7 Utility Class 8', 'T7IS']\n",
    "EMFACT_classes_bus = list(busVehicleClassDict.keys())\n",
    "EMFACT_classes = EMFACT_classes_car + EMFACT_classes_bus + EMFACT_classes_freight\n",
    "\n",
    "county_dict = {\n",
    "        'Alameda (SF)': 'Alameda County',\n",
    "        'Contra Costa (SF)': 'Contra Costa County',\n",
    "        'Marin (SF)': 'Marin County',\n",
    "        'Napa (SF)': 'Napa County',\n",
    "        'San Francisco (SF)': 'San Francisco County',\n",
    "        'San Mateo (SF)': 'San Mateo County', \n",
    "        'Santa Clara (SF)': 'Santa Clara County',\n",
    "        'Solano (SF)': 'Solano County',\n",
    "        'Sonoma (SF)': 'Sonoma County'\n",
    "    }\n",
    "\n",
    "events_column_types = {\n",
    "    'type': 'category',\n",
    "    'vehicle': 'str',\n",
    "    'links': 'str',  \n",
    "    'linkTravelTime': 'str',\n",
    "    'mode': 'str',\n",
    "    'vehicleType': 'category',\n",
    "    'departureTime': 'Int64', \n",
    "    'length': 'float'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################################################\n",
    "########################## FUNCTIONS ########################\n",
    "#############################################################\n",
    "\n",
    "def addGeometryIdToDataFrame(df, gdf, xcol, ycol, idColumn=\"geometry\", df_geom='EPSG:32610', column='blkgrpid', df_geom2='EPSG:4326'):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  addGeometryIdToDataFrame ...')\n",
    "\n",
    "    # Spatial join of points with polygons\n",
    "    \n",
    "    gdf.crs = df_geom2\n",
    "    gdf_data = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df[xcol], df[ycol]))\n",
    "    gdf_data.crs = df_geom  # Updated to use EPSG code directly\n",
    "    joined = gpd.sjoin(gdf_data.to_crs('EPSG:26910'), gdf.to_crs('EPSG:26910'), how='inner')\n",
    "    gdf_data = gdf_data.merge(joined[column], left_index=True, right_index=True, how=\"left\")\n",
    "    gdf_data.rename(columns={column: idColumn}, inplace=True)\n",
    "    df = pd.DataFrame(gdf_data.drop(columns='geometry'))\n",
    "\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return df.loc[~df.index.duplicated(keep='first'), :]\n",
    "\n",
    "def split_row(row):\n",
    "    \n",
    "    # Split a Network link into smaller segments \n",
    "    # - Adjust link length and coordinates\n",
    "    # - Add segment part code\n",
    "    \n",
    "    global segment_length\n",
    "\n",
    "    num_segments = int(np.ceil(row['linkLength'] / segment_length))\n",
    "    rows = []\n",
    "    for i in range(num_segments):\n",
    "        segment_length_eff = row['linkLength'] / num_segments\n",
    "        rows.append({\n",
    "            **row,\n",
    "            'fromLocationX': row['fromLocationX'] + (row['toLocationX'] - row['fromLocationX']) * (i / num_segments),\n",
    "            'fromLocationY': row['fromLocationY'] + (row['toLocationY'] - row['fromLocationY']) * (i / num_segments),\n",
    "            'toLocationX': row['fromLocationX'] + (row['toLocationX'] - row['fromLocationX']) * ((i + 1) / num_segments),\n",
    "            'toLocationY': row['fromLocationY'] + (row['toLocationY'] - row['fromLocationY']) * ((i + 1) / num_segments),\n",
    "            'linkLength': segment_length_eff,\n",
    "            'segment_part': i + 1\n",
    "        })\n",
    "\n",
    "    return rows\n",
    "\n",
    "#############################################################\n",
    "########################## NETWORK ##########################\n",
    "#############################################################\n",
    "\n",
    "def split_network(network):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  split_network ...')\n",
    "    \n",
    "    # Split network database on smaller links (segment_length) to better associate with ISRM borders\n",
    "\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return pd.DataFrame([y for x in network.apply(split_row, axis=1) for y in x])\n",
    "\n",
    "def calculate_midpoints(network):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  calculate_midpoints ...')\n",
    "\n",
    "    # Get link centroids for all network links\n",
    "\n",
    "    network['X'] = (network['fromLocationX'] + network['toLocationX']) / 2\n",
    "    network['Y'] = (network['fromLocationY'] + network['toLocationY']) / 2\n",
    "\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return network\n",
    "\n",
    "def enrich_geo_data(network, ISRM_grid, BGs, block_info, counties_shapefile):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  enrich_geo_data ...')\n",
    "\n",
    "    # Add geographic information to the network\n",
    "    # - ISRM\n",
    "    # - Block ID\n",
    "    # - Block info: county, city, etc.\n",
    "\n",
    "    network = addGeometryIdToDataFrame(network, ISRM_grid, 'X', 'Y', 'ISRM', column='isrm', \n",
    "                                       df_geom2='+proj=lcc +lat_0=40 +lon_0=-97 +lat_1=33 +lat_2=45 +x_0=0 +y_0=0 +ellps=sphere +units=m +no_defs +type=crs')\n",
    "    network = addGeometryIdToDataFrame(network, BGs, 'X', 'Y', 'BlockGroup', column='blkgrpid')\n",
    "    network = pd.merge(network, block_info, how='left', left_on='BlockGroup', right_on='bgid')\n",
    "    network['linkId'] = network['linkId'].astype(int)\n",
    "\n",
    "    def recalculate_counties(network, counties_shapefile):\n",
    "    # Print the initial dataframe and its keys\n",
    "        print(network.county_name.value_counts())\n",
    "\n",
    "        # Count NaN values in the county_name column before the update\n",
    "        if 'county_name' in network.columns:\n",
    "            network = network.drop(columns=['county_name'])\n",
    "        else:\n",
    "            print(\"'county_name' column does not exist before update.\")\n",
    "\n",
    "        network = addGeometryIdToDataFrame(network, counties_shapefile, xcol='X', ycol='Y', \n",
    "                                           idColumn='county_name', df_geom='EPSG:26910', \n",
    "                                           column='coname', df_geom2='EPSG:4326')\n",
    "\n",
    "        # Print the updated dataframe and its keys\n",
    "        print(network.county_name.value_counts())\n",
    "\n",
    "\n",
    "        # Count NaN values in the county_name column after the update\n",
    "        print(f\"Number of NaN values in 'county_name' after update: {network['county_name'].isna().sum()}\")\n",
    "    \n",
    "        return network\n",
    "    network = recalculate_counties(network, counties_shapefile)\n",
    "    network['county_name'] = network['county_name'] + ' County'\n",
    "\n",
    "\n",
    "    \n",
    "#     columns_to_fill = ['bgid', 'tractid', 'juris_name', 'mpo']\n",
    "#     network[columns_to_fill] = network[columns_to_fill].fillna('Other')\n",
    "#     network['county_name'] = network['county_name'].fillna(other_county)\n",
    "\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return network\n",
    "\n",
    "#############################################################\n",
    "########################## PATHTRAVERSAL ####################\n",
    "#############################################################\n",
    "\n",
    "def filter_events(events):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  filter_pathtraversal ...')\n",
    "    \n",
    "    # Filter PathTraversal events from the event file\n",
    "    \n",
    "    initial_len = len(events)\n",
    "    pathTraversal = events[events['type'] == 'PathTraversal'].copy()\n",
    "    pathTraversal.loc[:, 'PTID'] = pathTraversal.index\n",
    "    print('     ... Len Events', initial_len)\n",
    "    print('     ... Len pathTraversal', len(pathTraversal))\n",
    "    print('     ... Total Length in Meters PathTraversal', pathTraversal.length.sum())\n",
    "\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return pathTraversal\n",
    "\n",
    "def drop_na_links(pathTraversal):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  drop_na_links ...')\n",
    "\n",
    "    # Drop PathTraversal with no links\n",
    "\n",
    "    initial_len = pathTraversal.length.sum()\n",
    "    initial_count = len(pathTraversal)\n",
    "    pathTraversal = pathTraversal.dropna(subset=['links'])\n",
    "    print('     ... Total Length in Meters PathTraversal', initial_len)\n",
    "    print('     ... Len pathTraversal', initial_count)\n",
    "    print('     ... Total Length in Meters PathTraversal after dropping Nan links', pathTraversal.length.sum())\n",
    "    print('     ... Len pathTraversal after dropping Nan links',len(pathTraversal))\n",
    "\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return pathTraversal\n",
    "\n",
    "def filter_modes(pathTraversal, scenario):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  filter_modes ...')\n",
    "    \n",
    "    # Filter PathTraversal modes\n",
    "    \n",
    "    global pathTraversalModesDict\n",
    "    \n",
    "    print('     ... Modes of pathTraversal', pathTraversal['mode'].value_counts().keys())\n",
    "\n",
    "    pathTraversal['vehicleType'].value_counts().to_csv(f'vehicletypes_{scenario}.csv')\n",
    "    pathTraversal.loc[\n",
    "        pathTraversal['vehicleType'].str.contains(r'T6|T7', na=False),\n",
    "        'mode'\n",
    "        ] = 'freight'\n",
    "    pathTraversal = pathTraversal[pathTraversal['mode'].isin(pathTraversalModesDict.keys())].copy()\n",
    "    print('     ... Len pathTraversal after dropping active modes', len(pathTraversal))\n",
    "    print('     ... Total Length in Meters PathTraversal after dropping active modes',pathTraversal.length.sum())\n",
    " \n",
    "    pathTraversal.loc[:, 'mode'] = pathTraversal['mode'].map(pathTraversalModesDict)\n",
    "    \n",
    "    print('     ... Total Length in Meters PathTraversal of freight vehicles',\n",
    "          pathTraversal[pathTraversal['mode']=='freight'].length.sum())\n",
    "    print('     ... Total Length in Meters PathTraversal of car vehicles',\n",
    "          pathTraversal[pathTraversal['mode']=='car'].length.sum())\n",
    "    print('     ... Total Length in Meters PathTraversal of bus vehicles',\n",
    "          pathTraversal[pathTraversal['mode']=='UBUS'].length.sum())\n",
    "    print('     ... Len pathTraversal Freight', len(pathTraversal[pathTraversal['mode'].isin(['freight'])]))\n",
    "    print('     ... Len pathTraversal Car', len(pathTraversal[pathTraversal['mode'].isin(['car'])]))\n",
    "    print('     ... Len pathTraversal Bus', len(pathTraversal[pathTraversal['mode'].isin(['UBUS'])]))\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return pathTraversal\n",
    "\n",
    "def process_links(pathTraversal):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  process_links ...')\n",
    "    \n",
    "    # - Adjust format of linkIDs and their travel times for each PathTraversal\n",
    "    # - Get first and last link for each PathTraversal and filter Nan\n",
    "    \n",
    "    global correction_factor_duration\n",
    "\n",
    "    pathTraversal['links'] = pathTraversal['links'].str.split(',')\n",
    "    pathTraversal['linkTravelTime'] = pathTraversal['linkTravelTime'].str.split(',')\n",
    "    pathTraversal['linkTravelTime'] = pathTraversal['linkTravelTime'].apply(\n",
    "        lambda times: [str(float(time) * correction_factor_duration) for time in times])\n",
    "    pathTraversal['firstLink'] = pathTraversal['links'].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
    "    pathTraversal['lastLink'] = pathTraversal['links'].apply(lambda x: x[-1] if isinstance(x, list) and len(x) > 0 else None)\n",
    "    pathTraversal.dropna(subset=['firstLink', 'lastLink'], inplace=True)\n",
    "    print('     ... Total Length in Meters PathTraversal after dropping rows with None in firstLink or lastLink', pathTraversal.length.sum())\n",
    "    print('     ... Len pathTraversal after dropping rows with None in firstLink or lastLink', len(pathTraversal))\n",
    "\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return pathTraversal\n",
    "\n",
    "def calculate_link_departure_times(departureTime, linkTravelTimes):\n",
    "    \n",
    "    # For a specific PathTraversal calculate the departure time of each traveled link based\n",
    "    # on trip departure and link travel times\n",
    "    \n",
    "    link_departure_times = [departureTime]\n",
    "    current_time = departureTime\n",
    "    for travel_time in linkTravelTimes[:-1]:\n",
    "        current_time += float(travel_time) \n",
    "        link_departure_times.append(current_time) \n",
    "\n",
    "    return link_departure_times\n",
    "\n",
    "def link_times(pathTraversal):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  link_times ...')\n",
    "\n",
    "    # - Calculate departing times for each link in a PathTraversal\n",
    "    # - For each PathTraversal, get a list comprehending the sequence of linkIDs, Travel times and departing times\n",
    "    \n",
    "    pathTraversal['link_departure_times'] = pathTraversal.apply(\n",
    "        lambda row: calculate_link_departure_times(row['departureTime'], row['linkTravelTime']), axis=1)\n",
    "    \n",
    "    pathTraversal['link_with_time'] = pathTraversal.apply(\n",
    "        lambda row: list(zip(row['links'], row['linkTravelTime'], row['link_departure_times'])), axis=1)\n",
    "    \n",
    "    pathTraversal = pathTraversal.drop(['links', 'linkTravelTime', 'link_departure_times'], axis=1)\n",
    "\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return pathTraversal\n",
    "\n",
    "#############################################################\n",
    "########################## PARKING ##########################\n",
    "#############################################################\n",
    "\n",
    "def prepare_park_departDatabase(pathTraversal):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  prepare_park_departDatabase ...')\n",
    "\n",
    "    # For each PathTraversal, get parking time\n",
    "    # - Last one will go until end_day_parking\n",
    "    \n",
    "    global end_day_parking\n",
    "    \n",
    "    pathTraversal['trip_duration'] = pathTraversal['link_with_time'].apply(\n",
    "        lambda x: sum(float(triplet[1]) for triplet in x)\n",
    "    )\n",
    "    pathTraversal['start_parking_time'] = pathTraversal['departureTime'] + pathTraversal['trip_duration'] \n",
    "    pathTraversal.sort_values(by=['vehicle', 'departureTime'], inplace=True)\n",
    "    pathTraversal['next_departureTime'] = pathTraversal.groupby('vehicle')['departureTime'].shift(-1)\n",
    "    pathTraversal['next_vehicle'] = pathTraversal['vehicle'].shift(-1)\n",
    "    pathTraversal['parking_time'] = pathTraversal.apply(\n",
    "        lambda x: (x['next_departureTime'] - x['start_parking_time']) if x['vehicle'] == x['next_vehicle'] else max(0, end_day_parking - x['start_parking_time'] ), axis=1\n",
    "    )\n",
    "    pathTraversal = pathTraversal.dropna(subset=['parking_time', 'departureTime'])\n",
    "\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return pathTraversal\n",
    "\n",
    "def prepare_parkingDatabase(pathTraversal, networkParking):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  prepare_parkingDatabase ...')\n",
    "\n",
    "    # Add parking time prior to first trip to cover the 24h parking time\n",
    "    # - Filter low parking times, due to small drop offs or when vehicles start looking for parking\n",
    "    # - Note: first row for each vehicle can be easily overlapped since it would be filtered anyway\n",
    "    # - Drop last parking time to get the prior-trip parking database\n",
    "    \n",
    "    global beginning_day_parking\n",
    "    global parking_limit\n",
    "    \n",
    "    pathTraversal.loc[pathTraversal.groupby('vehicle').cumcount() == 0, 'parking_time'] = (pathTraversal['departureTime'] - beginning_day_parking).clip(lower=0)\n",
    "    parkDepartDatabase = pathTraversal[pathTraversal['parking_time'] >= parking_limit].copy()\n",
    "    parkDepartDatabase.loc[:, 'lastLink']  = parkDepartDatabase['lastLink'].astype(int)\n",
    "    parkDepartDatabase = pd.merge(parkDepartDatabase, networkParking[['linkId','linkLength','ISRM','county_name']], how='left', left_on='lastLink', right_on='linkId')\n",
    "    \n",
    "    departDatabase = parkDepartDatabase.drop(parkDepartDatabase.groupby('vehicle').tail(1).index)\n",
    "    parkingDatabase_no_first = parkDepartDatabase.drop(parkDepartDatabase.groupby('vehicle').head(1).index)\n",
    "\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return parkDepartDatabase, departDatabase, parkingDatabase_no_first\n",
    "\n",
    "\n",
    "#############################################################\n",
    "########################## EXPLODE LINKS ####################\n",
    "#############################################################\n",
    "\n",
    "def explode_path_traversal(pathTraversal):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  explode_path_traversal ...')\n",
    "\n",
    "    # Explode PathTraversals link by link\n",
    "    \n",
    "    explodedPathTraversal = pathTraversal.explode('link_with_time')\n",
    "    explodedPathTraversal['identifier'] = range(len(explodedPathTraversal))\n",
    "    explodedPathTraversal[['link', 'travelTime', 'departureTime']] = pd.DataFrame(\n",
    "        explodedPathTraversal['link_with_time'].tolist(), index=explodedPathTraversal.index)\n",
    "    explodedPathTraversal = explodedPathTraversal.drop(['link_with_time'], axis=1)\n",
    "    explodedPathTraversal['travelTime'] = explodedPathTraversal['travelTime'].astype(float)\n",
    "    explodedPathTraversal['departureTime'] = explodedPathTraversal['departureTime'].astype(float)\n",
    "    explodedPathTraversal['link'] = explodedPathTraversal['link'].astype(int)\n",
    "\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return explodedPathTraversal\n",
    "\n",
    "def merge_with_network(explodedPathTraversal, networkExploded):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  merge_with_network ...')\n",
    "\n",
    "    # Merge PathTraversal with network to further explode links on smaller ones (see function split_network)\n",
    "    \n",
    "    networkExploded['linkId'] = networkExploded['linkId'].astype(int)\n",
    "    explodedPathTraversal = pd.merge(\n",
    "        explodedPathTraversal, networkExploded[['linkId', 'linkLength', 'bgid','ISRM', 'county_name', 'segment_part']],\n",
    "        how='left', left_on='link', right_on='linkId')\n",
    "    explodedPathTraversal = explodedPathTraversal.drop(['linkId'], axis=1)\n",
    "    \n",
    "    print('     ... Len explodedPathTraversal', len(explodedPathTraversal))\n",
    "    explodedPathTraversal = explodedPathTraversal.dropna(subset=['county_name'])\n",
    "    print('     ... Len explodedPathTraversal after dropping trip outside the simulated counties: ', len(explodedPathTraversal))\n",
    "\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return explodedPathTraversal\n",
    "\n",
    "def adjust_travel_times(explodedPathTraversal):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  adjust_travel_times ...')\n",
    "\n",
    "    # Edit travel and departing times accordingly to the link disaggregation\n",
    "    \n",
    "    explodedPathTraversal['total_length'] = explodedPathTraversal.groupby(\n",
    "        ['vehicle', 'firstLink', 'lastLink', 'identifier', 'PTID'])['linkLength'].transform('sum')\n",
    "    explodedPathTraversal['length_proportion'] = explodedPathTraversal['linkLength'] / explodedPathTraversal['total_length']\n",
    "    explodedPathTraversal['travelTime'] *= explodedPathTraversal['length_proportion']\n",
    "    explodedPathTraversal['cumulativeTravelTime'] = explodedPathTraversal.groupby(\n",
    "        ['vehicle', 'firstLink', 'lastLink', 'identifier', 'PTID'])['travelTime'].cumsum()\n",
    "    explodedPathTraversal['departureTime'] += explodedPathTraversal.groupby(\n",
    "        ['vehicle', 'firstLink', 'lastLink', 'identifier', 'PTID'])['cumulativeTravelTime'].shift(fill_value=0)\n",
    "    explodedPathTraversal = explodedPathTraversal.drop(['total_length', 'length_proportion', 'cumulativeTravelTime'], axis=1)\n",
    "\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return explodedPathTraversal\n",
    "\n",
    "def calculate_speeds(explodedPathTraversal):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  calculate_speeds ...')\n",
    "\n",
    "    # Add Speed column and adjust units\n",
    "    \n",
    "    global default_speed\n",
    "    \n",
    "    explodedPathTraversal['linkLength'] /= 1000 * 1.60934\n",
    "    explodedPathTraversal['travelTime'] /= 3600\n",
    "    explodedPathTraversal['average_speed_mph'] = explodedPathTraversal['linkLength'] / explodedPathTraversal['travelTime']\n",
    "    print('     ... Len explodedPathTraversal', len(explodedPathTraversal))\n",
    "    print('     ... Len of undetermined speeds:', len(explodedPathTraversal.loc[(explodedPathTraversal['travelTime'] == 0) | (explodedPathTraversal['linkLength'] == 0), 'average_speed_mph']))\n",
    "    explodedPathTraversal.loc[(explodedPathTraversal['travelTime'] == 0) | (explodedPathTraversal['linkLength'] == 0), 'average_speed_mph'] = default_speed\n",
    "\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return explodedPathTraversal\n",
    "\n",
    "#############################################################\n",
    "################# ASSIGN EMFACT VEHICLE TYPES ###############\n",
    "#############################################################\n",
    "\n",
    "\n",
    "def load_EMFACT_data(filepath, EMFACT_classes):\n",
    "    start_time = time.time()\n",
    "    print(' - Function:  load_EMFACT_data ...')\n",
    "\n",
    "    global county_dict\n",
    "\n",
    "    EMFACT_file = pd.read_csv(filepath)\n",
    "    print(f\"   Loaded {len(EMFACT_file):,} rows from {filepath}\")\n",
    "\n",
    "    # Check unaccounted BEFORE filtering\n",
    "    unaccounted = EMFACT_file.loc[~EMFACT_file['vehicle_class'].isin(EMFACT_classes), 'vehicle_class'].unique()\n",
    "    if len(unaccounted):\n",
    "        print(f\"   Unaccounted vehicle classes ({len(unaccounted)}): {unaccounted}\")\n",
    "    else:\n",
    "        print(\"   All vehicle classes are accounted for.\")\n",
    "\n",
    "    # Filter\n",
    "    EMFACT_file = EMFACT_file[EMFACT_file['vehicle_class'].isin(EMFACT_classes)]\n",
    "    print(f\"   Remaining after mode filter: {len(EMFACT_file):,} rows\")\n",
    "\n",
    "    # Map county names\n",
    "    EMFACT_file['sub_area'] = EMFACT_file['sub_area'].map(county_dict)\n",
    "    EMFACT_file = EMFACT_file[EMFACT_file['sub_area'].isin(list(county_dict.values()))]\n",
    "    print(f\"   After county filter: {len(EMFACT_file):,} rows in {EMFACT_file['sub_area'].nunique()} counties\")\n",
    "\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return EMFACT_file\n",
    "\n",
    "def calculate_vmt_distribution(EMFACT_vmt):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  calculate_vmt_distribution by county ...')\n",
    "\n",
    "    # Get EMFACT VMT distribution per vehicle type and county, weighted on VMT\n",
    "    \n",
    "    total_vmt_by_category = EMFACT_vmt.groupby(['sub_area', 'vehicle_class', 'fuel']).agg({'total_vmt': 'sum'}).reset_index()\n",
    "    totalVMTbyCounty = total_vmt_by_category.groupby('sub_area').agg({'total_vmt': 'sum'}).rename(columns={'total_vmt': 'total_vmt_county'})\n",
    "    totalVMTbyCounty = total_vmt_by_category.merge(totalVMTbyCounty, left_on='sub_area', right_index=True)\n",
    "    totalVMTbyCounty['vmt_distribution'] = totalVMTbyCounty['total_vmt'] / totalVMTbyCounty['total_vmt_county']\n",
    "#     totalVMTbyCounty['cdf'] = totalVMTbyCounty.groupby('sub_area')['vmt_distribution'].cumsum()\n",
    "    print(f\"   Created VMT distribution for {totalVMTbyCounty['sub_area'].nunique()} counties\")\n",
    "    print(\"   Example distribution rows:\")\n",
    "    print(totalVMTbyCounty.head())\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return totalVMTbyCounty\n",
    "\n",
    "\n",
    "def create_vmtLookup(totalVMTbyCounty):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  create_vmtLookup ...')\n",
    "\n",
    "    # Create a dictionary of VMT distribution per vehicle type and county\n",
    "    \n",
    "    vmtLookup = {}\n",
    "    for sub_area, group in totalVMTbyCounty.groupby('sub_area'):\n",
    "        vmtLookup[sub_area] = {\n",
    "            'vmt_distribution': group['vmt_distribution'].values,\n",
    "            'vehicle_classes': group['vehicle_class'].values,\n",
    "            'fuels': group['fuel'].values\n",
    "        }\n",
    "    print(f\"   Total counties in lookup: {len(vmtLookup)}\")\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return vmtLookup\n",
    "\n",
    "\n",
    "def assign_subset(sub_area_data, vmtLookup):\n",
    "    \n",
    "#     print(' - Function:  assign_subset ...')\n",
    "\n",
    "    # For each car-related explodedPathTraversal of a specific county, assign vehicles based \n",
    "    # on the EMFACT VMT distribution\n",
    "    \n",
    "    global seed\n",
    "    \n",
    "    sub_area = sub_area_data['county_name'].iloc[0]\n",
    "    lookup = vmtLookup[sub_area]\n",
    "    total_rows = len(sub_area_data)\n",
    "    print(f\"   Assigning {total_rows} rows for county '{sub_area}' from {len(lookup['vehicle_classes'])} EMFACT classes\")\n",
    "    assignments = []\n",
    "    for threshold, vehicle_class, fuel in zip(lookup['vmt_distribution'], lookup['vehicle_classes'], lookup['fuels']):\n",
    "        num_rows = int(round(threshold * total_rows+1))\n",
    "        assignments += [(vehicle_class, fuel)] * num_rows\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(assignments)\n",
    "    assignments = assignments[:total_rows]\n",
    "    sub_area_data.loc[:, 'vehicle_class_emfact'], sub_area_data.loc[:, 'fuel_type_emfact'] = zip(*assignments)\n",
    "    \n",
    "    return sub_area_data\n",
    "\n",
    "def add_emfact_vehicle_type_buses(bus_group):\n",
    "    \n",
    "#     print(' - Function:  assign_subset ...')\n",
    "\n",
    "    # For each car-related explodedPathTraversal of a specific county, assign vehicles based \n",
    "    # on the EMFACT VMT distribution\n",
    "\n",
    "    bus_group.loc[:, 'vehicle_class_emfact'] = bus_group.loc[:, 'mode']\n",
    "    bus_group.loc[:, 'fuel_type_emfact'] = busesFuelType\n",
    "    \n",
    "    return bus_group\n",
    "\n",
    "def assign_vehicle_classes(dataframe, car_vmtLookup, freight_vmtLookup):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  assign_vehicle_classes ...')\n",
    "\n",
    "    # Update car-related modes based on EMFACT distribution (vmtLookup)\n",
    "            \n",
    "    final_result = pd.DataFrame()\n",
    "    print(f'len dataframe: {len(dataframe)}')\n",
    "    for name, group in dataframe.groupby('county_name'):\n",
    "        car_group = group[group['mode'] == 'car'].copy()\n",
    "        bus_group = group[group['mode'].isin(EMFACT_classes_bus)].copy()\n",
    "        freight_group = group[group['mode'] == 'freight'].copy()\n",
    "        print(f'{name} - len car dataframe: {len(car_group)}')\n",
    "        print(f'{name} - len freight dataframe: {len(freight_group)}')\n",
    "        print(f'{name} - len bus dataframe: {len(bus_group)}')\n",
    "        bus_group = add_emfact_vehicle_type_buses(bus_group)\n",
    "        final_result = pd.concat([final_result, assign_subset(car_group, car_vmtLookup),\n",
    "                                  assign_subset(freight_group, freight_vmtLookup), \n",
    "                                  bus_group])\n",
    "\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return final_result\n",
    "\n",
    "def correct_cruise_vehicle_classes(dataframe):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  correct_cruise_vehicle_classes ...')\n",
    "\n",
    "    # Update Cruise vehicle type\n",
    "    dataframe.loc[dataframe.vehicle.str.contains('@Cruise'), 'fuel_type_emfact'] = cruise_fuel_type\n",
    "    dataframe.loc[dataframe.vehicle.str.contains('@Cruise'), 'vehicle_class_emfact'] = cruise_class_type\n",
    "\n",
    "    \n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return dataframe\n",
    "\n",
    "#############################################################\n",
    "############### ACTIVITIES CORRECTION FACTORS ###############\n",
    "#############################################################\n",
    "\n",
    "def calculate_correction_factors(explodedPathTraversal, departDatabase, EMFACT_vmt, EMFACT_trips, scale, scenario):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  calculate_correction_factors ...')\n",
    "\n",
    "    # Get activity-based correction factors by county and vehicle type, based on VMT and #Trips activities\n",
    "    \n",
    "    global EMFACT_classes_bus\n",
    "#     explodedPathTraversal.loc[explodedPathTraversal['bgid'] == 'Other', 'county_name'] = 'Other'\n",
    "    print(type(scale), scale)\n",
    "\n",
    "    vmt_by_county_BEAM = explodedPathTraversal.groupby(['county_name', 'vehicle_class_emfact'])['linkLength'].sum() / scale\n",
    "    vmt_by_county_EMFACT = EMFACT_vmt.groupby(['sub_area', 'vehicle_class'])['total_vmt'].sum()\n",
    "    vmt_by_county_BEAM.index.rename(['sub_area', 'vehicle_class'], inplace=True)\n",
    "    corr_VMT_by_county = (vmt_by_county_BEAM).div(vmt_by_county_EMFACT, fill_value=np.nan).fillna(1)\n",
    "    corr_VMT_by_county_dict = corr_VMT_by_county.to_dict()\n",
    "\n",
    "    trips_by_county_BEAM_origin = departDatabase.groupby(['county_name','vehicle_class_emfact']).county_name.count() / scale\n",
    "    trips_by_county_EMFACT = EMFACT_trips.groupby(['sub_area', 'vehicle_class'])['trips'].sum()\n",
    "    trips_by_county_BEAM_origin.index.rename(['sub_area', 'vehicle_class'], inplace=True)\n",
    "    corr_Trips_by_county = (trips_by_county_BEAM_origin).div(trips_by_county_EMFACT, fill_value=np.nan).fillna(1)\n",
    "    corr_Trips_by_county_dict = corr_Trips_by_county.to_dict()\n",
    "    \n",
    "#     for (county, vehicle_class), factor in corr_VMT_by_county_dict.items():\n",
    "#         if vehicle_class in EMFACT_classes_bus:\n",
    "#             corr_VMT_by_county_dict[(county, vehicle_class)] *= scale\n",
    "\n",
    "#     for (county, vehicle_class), factor in corr_Trips_by_county_dict.items():\n",
    "#         if vehicle_class in EMFACT_classes_bus:\n",
    "#             corr_Trips_by_county_dict[(county, vehicle_class)] *= scale\n",
    "        \n",
    "    print('corr_VMT_by_county_dict', corr_VMT_by_county_dict )\n",
    "    print('corr_Trips_by_county_dict', corr_Trips_by_county_dict )\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    \n",
    "    pd.DataFrame.from_dict(corr_VMT_by_county_dict, orient='index').to_csv(f'{scenario}_corr_VMT_by_county.csv')\n",
    "    pd.DataFrame.from_dict(corr_Trips_by_county_dict, orient='index').to_csv(f'{scenario}_corr_Trips_by_county.csv')\n",
    "    return corr_VMT_by_county_dict, corr_Trips_by_county_dict\n",
    "\n",
    "\n",
    "def apply_correction_factors(df, correction_factors, correctionFactorName):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  apply_correction_factors ...')\n",
    "\n",
    "    # Assign correction factors to the databases\n",
    "    \n",
    "    correction_factors_series = pd.Series(correction_factors)\n",
    "#     print(correction_factors_series)\n",
    "#     df['county_vehicle_key'] = df['county_name'] + '_' + df['vehicle_class_emfact']\n",
    "#     df[correctionFactorName] = df['county_vehicle_key'].map(correction_factors_series).fillna(1)\n",
    "#     df.drop('county_vehicle_key', axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    df['combined_key'] = list(zip(df['county_name'], df['vehicle_class_emfact']))\n",
    "    df[correctionFactorName] = df['combined_key'].map(correction_factors_series).fillna(1)\n",
    "    df.drop('combined_key', axis=1, inplace=True)\n",
    "    \n",
    "#         df['corr_VMT_by_county'] = df.apply(lambda row: corr_VMT_by_county_dict.get((row['county_name'], row['vehicle_class_emfact']), 1), axis=1)\n",
    "#         df['corr_trips_by_county'] = df.apply(lambda row: corr_Trips_by_county_dict.get((row['county_name'], row['vehicle_class_emfact']), 1), axis=1)\n",
    "#     print(df[correctionFactorName].value_counts())\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "#############################################################\n",
    "####################### EMISSION FACTORS ####################\n",
    "#############################################################\n",
    "\n",
    "def rename_emfact_columns(emfactEmisRates):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  rename_emfact_columns ...')\n",
    "\n",
    "    # Rename EMFACT columns\n",
    "    \n",
    "    emfactEmisRates.rename(columns={'fuel': 'fuel_type_emfact', 'speed_time': 'average_speed_mph', 'sub_area': 'county_name', 'vehicle_class': 'vehicle_class_emfact'}, inplace=True)\n",
    "\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return emfactEmisRates\n",
    "\n",
    "def categorize_speed_data(explodedPathTraversal):\n",
    "    \n",
    "    def categorize_speed(speed):\n",
    "\n",
    "#         print(' - Function:  categorize_speed ...')\n",
    "\n",
    "        #Categorize speed on EMFACT speed bins\n",
    "\n",
    "        category = min((speed // 5) * 5 + 5, 90)\n",
    "        return max(category, 5)\n",
    "    \n",
    "    explodedPathTraversal['average_speed_mph'] = explodedPathTraversal['average_speed_mph'].apply(categorize_speed)\n",
    "    \n",
    "    return explodedPathTraversal\n",
    "\n",
    "\n",
    "def adjust_departDatabase_speed(departDatabase):\n",
    "    \n",
    "    def categorize_park(park):\n",
    "\n",
    "#         print(' - Function:  categorize_park ...')\n",
    "\n",
    "        #Categorize park time on EMFACT time bins\n",
    "\n",
    "        intervals = [5.0, 10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 120.0, 180.0, 240.0, 300.0, 360.0, 420.0, 480.0, 540.0, 600.0, 660.0, 720.0]\n",
    "        pos = bisect.bisect_left(intervals, park)\n",
    "        return intervals[min(pos, len(intervals) - 1)]\n",
    "\n",
    "    departDatabase['average_speed_mph'] = departDatabase['parking_time'] / 60  # Parking time is on average speed column for STREX\n",
    "    departDatabase['average_speed_mph'] = departDatabase['average_speed_mph'].apply(categorize_park)\n",
    "    \n",
    "    return departDatabase\n",
    "\n",
    "#############################################################\n",
    "################## ASSIGN EMISSION FACTORS ##################\n",
    "#############################################################\n",
    "\n",
    "def process_emission_rates(emfactEmisRates, explodedPathTraversal, parkingDatabase, departDatabase, parkingDatabase_no_first, scale):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  process_emission_rates ...')\n",
    "\n",
    "    # Associate emisison rates to the right dataframe\n",
    "    \n",
    "    global processDataframes \n",
    "    global processes \n",
    "    global pollutants_list \n",
    "    global merging_lists \n",
    "    \n",
    "    for process, pollutants, merging_list, processdf in zip(processes, pollutants_list, merging_lists, processDataframes):\n",
    "        print(' + Process:', process)\n",
    "        \n",
    "        for pollutant in pollutants:\n",
    "            print('        - Pollutant:', pollutant)\n",
    "            process_emission_rates = emfactEmisRates[emfactEmisRates.process == process]\n",
    "            pollutant_emission_rates = process_emission_rates[process_emission_rates['pollutant'] == pollutant]\n",
    "            print('                 ... LEN emfact emis factors: ', len(pollutant_emission_rates))\n",
    "            pollutant_emission_rates = pollutant_emission_rates.drop_duplicates(subset=merging_list)\n",
    "            print('                 ... LEN emfact emis factors after dropping duplicates: ', len(pollutant_emission_rates))\n",
    "            pollutant_emission_rates.rename(columns={'emission_rate': f'emission_rate_{process}_{pollutant}'}, inplace=True)\n",
    "            \n",
    "            def process_emissions_for_df(df, process, pollutant, merging_list, pollutant_emission_rates, scale):\n",
    "                start_time = time.time() \n",
    "    \n",
    "                print(' - Function:  process_emissions_for_df ...')\n",
    "                df = pd.merge(df, pollutant_emission_rates[merging_list + [f'emission_rate_{process}_{pollutant}']],\n",
    "                              on=merging_list, how='left')\n",
    "                df = calculate_emissions(df, process, pollutant, scale)\n",
    "                print(f'           ... Emission added:', sum(df[f'tons_per_year_{process}_{pollutant}'].fillna(0)), 'tons per year')\n",
    "                \n",
    "                print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "                return df\n",
    "            \n",
    "            if processdf == 'PT':\n",
    "                explodedPathTraversal = process_emissions_for_df(explodedPathTraversal, process, pollutant, merging_list, pollutant_emission_rates, scale)\n",
    "            elif processdf == 'PAn':\n",
    "                parkingDatabase_no_first = process_emissions_for_df(parkingDatabase_no_first, process, pollutant, merging_list, pollutant_emission_rates, scale)\n",
    "            elif processdf == 'PA':\n",
    "                parkingDatabase = process_emissions_for_df(parkingDatabase, process, pollutant, merging_list, pollutant_emission_rates, scale)\n",
    "            elif processdf == 'DP':\n",
    "                departDatabase = process_emissions_for_df(departDatabase, process, pollutant, merging_list, pollutant_emission_rates, scale)\n",
    "    \n",
    "\n",
    "\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return explodedPathTraversal, parkingDatabase, departDatabase, parkingDatabase_no_first\n",
    "\n",
    "def calculate_emissions(df, process, pollutant, scale):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  calculate_emissions ...')\n",
    "\n",
    "    # Muliply Emission Factors for the specific process and pollutant\n",
    "    \n",
    "    global days\n",
    "    global EMFACT_classes_bus\n",
    "    \n",
    "    df[f'tons_per_year_{process}_{pollutant}'] = df[f'emission_rate_{process}_{pollutant}'] * days / scale / 1000000 \n",
    "                                                      \n",
    "    if process in ['RUNEX', 'PMBW', 'PMTW']:\n",
    "        df[f'tons_per_year_{process}_{pollutant}'] *= (df['linkLength'] / df['corr_VMT_by_county'])\n",
    "    elif process == 'RUNLOSS':\n",
    "        df[f'tons_per_year_{process}_{pollutant}'] *= (df['travelTime'] / df['corr_VMT_by_county'])\n",
    "    elif process == 'HOTSOAK':\n",
    "        df[f'tons_per_year_{process}_{pollutant}'] /= df['corr_trips_by_county']\n",
    "    elif process == 'DIURN':\n",
    "        df[f'tons_per_year_{process}_{pollutant}'] *= (df['parking_time'] / 3600 / df['corr_trips_by_county'])\n",
    "    elif process == 'STREX':\n",
    "        df[f'tons_per_year_{process}_{pollutant}'] /=  df['corr_trips_by_county']\n",
    "        \n",
    "    df.loc[df['vehicle_class_emfact'].isin(EMFACT_classes_bus), f'tons_per_year_{process}_{pollutant}'] *= scale\n",
    "\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return df\n",
    "\n",
    "#############################################################\n",
    "#################### AGGREGATE RESULTS ######################\n",
    "#############################################################\n",
    "\n",
    "def aggregate_emissions(explodedPathTraversal, parkingDatabase, departDatabase, parkingDatabase_no_first):\n",
    "    start_time = time.time()\n",
    "#     def aggregate_df_emissions_by_isrm(dataframe, processes, pollutants_list):\n",
    "         \n",
    "    \n",
    "#         print(' - Function:  aggregate_emissions_by_isrm ...')\n",
    "\n",
    "#         # Aggregate emissions by ISRM\n",
    "\n",
    "#         emissions_aggregated = dataframe.groupby('ISRM')[\n",
    "#             [f'tons_per_year_{process}_{pollutant}' \n",
    "#              for pollutants, process in zip(pollutants_list, processes)\n",
    "#              for pollutant in pollutants \n",
    "#             ]\n",
    "#         ].sum().reset_index()\n",
    "#         return emissions_aggregated\n",
    "    \n",
    "    def aggregate_df_emissions_by_isrm_and_mode(dataframe, processes, pollutants_list):\n",
    "            print(' - Function:  aggregate_emissions_by_isrm_and_mode ...')\n",
    "            group_cols = ['ISRM', 'mode', 'county_name', 'vehicle_class_emfact']\n",
    "            emissions_aggregated = dataframe.groupby(group_cols)[\n",
    "                [f'tons_per_year_{process}_{pollutant}'\n",
    "                 for pollutants, process in zip(pollutants_list, processes)\n",
    "                 for pollutant in pollutants]\n",
    "            ].sum().reset_index()\n",
    "            return emissions_aggregated\n",
    "    \n",
    "    BEAM_emis = aggregate_df_emissions_by_isrm_and_mode(explodedPathTraversal, \n",
    "                                        ['RUNEX', 'PMBW', 'PMTW', 'RUNLOSS'], \n",
    "                                        [['PM2_5', 'SOx', 'NOx', 'ROG', 'NH3', 'CO2'], ['PM2_5'], ['PM2_5'], ['ROG']])\n",
    "    BEAM_emis_park_no_first = aggregate_df_emissions_by_isrm_and_mode(parkingDatabase_no_first, \n",
    "                                                          ['HOTSOAK'], \n",
    "                                                          [['ROG']])\n",
    "    BEAM_emis_park = aggregate_df_emissions_by_isrm_and_mode(parkingDatabase, \n",
    "                                                 ['DIURN'], \n",
    "                                                 [['ROG']])\n",
    "    BEAM_emis_dep = aggregate_df_emissions_by_isrm_and_mode(departDatabase, \n",
    "                                                ['STREX'], \n",
    "                                                [['PM2_5', 'SOx', 'NOx', 'ROG', 'CO2']])\n",
    "\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return BEAM_emis, BEAM_emis_park, BEAM_emis_park_no_first, BEAM_emis_dep\n",
    "\n",
    "def merge_emission_dataframes(df_list):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  merge_emission_dataframes ...')\n",
    "\n",
    "    # Merge emissions from different dataframes\n",
    "    \n",
    "    result = df_list[0]\n",
    "    for df in df_list[1:]:\n",
    "        result = pd.merge(result, df, on=['ISRM', 'mode', 'county_name', 'vehicle_class_emfact'], how='left')\n",
    "\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return result\n",
    "\n",
    "\n",
    "def calculate_total_emissions(emissions_df):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  calculate_total_emissions ...')\n",
    "\n",
    "    # Combining specific pollutants and processes into total emissions\n",
    "    \n",
    "    emissions_df['tons_per_year_ROG'] = emissions_df[['tons_per_year_RUNEX_ROG', 'tons_per_year_STREX_ROG', 'tons_per_year_DIURN_ROG', 'tons_per_year_HOTSOAK_ROG', 'tons_per_year_RUNLOSS_ROG']].sum(axis=1)\n",
    "    emissions_df['tons_per_year_PM2_5'] = emissions_df[['tons_per_year_RUNEX_PM2_5', 'tons_per_year_STREX_PM2_5', 'tons_per_year_PMBW_PM2_5', 'tons_per_year_PMTW_PM2_5']].sum(axis=1)\n",
    "    emissions_df['tons_per_year_SOx'] = emissions_df[['tons_per_year_RUNEX_SOx', 'tons_per_year_STREX_SOx']].sum(axis=1)\n",
    "    emissions_df['tons_per_year_NOx'] = emissions_df[['tons_per_year_RUNEX_NOx', 'tons_per_year_STREX_NOx']].sum(axis=1)\n",
    "    emissions_df['tons_per_year_NH3'] = emissions_df['tons_per_year_RUNEX_NH3']\n",
    "    emissions_df['tons_per_year_CO2'] = emissions_df[['tons_per_year_RUNEX_CO2', 'tons_per_year_STREX_CO2']].sum(axis=1)\n",
    "\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return emissions_df\n",
    "\n",
    "def calculate_total_emissions_noCO2(emissions_df):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  calculate_total_emissions ...')\n",
    "\n",
    "    # Combining specific pollutants and processes into total emissions\n",
    "    \n",
    "    emissions_df['tons_per_year_ROG'] = emissions_df[['tons_per_year_RUNEX_ROG', 'tons_per_year_STREX_ROG', 'tons_per_year_DIURN_ROG', 'tons_per_year_HOTSOAK_ROG', 'tons_per_year_RUNLOSS_ROG']].sum(axis=1)\n",
    "    emissions_df['tons_per_year_PM2_5'] = emissions_df[['tons_per_year_RUNEX_PM2_5', 'tons_per_year_STREX_PM2_5', 'tons_per_year_PMBW_PM2_5', 'tons_per_year_PMTW_PM2_5']].sum(axis=1)\n",
    "    emissions_df['tons_per_year_SOx'] = emissions_df[['tons_per_year_RUNEX_SOx', 'tons_per_year_STREX_SOx']].sum(axis=1)\n",
    "    emissions_df['tons_per_year_NOx'] = emissions_df[['tons_per_year_RUNEX_NOx', 'tons_per_year_STREX_NOx']].sum(axis=1)\n",
    "    emissions_df['tons_per_year_NH3'] = emissions_df['tons_per_year_RUNEX_NH3']\n",
    "#     emissions_df['tons_per_year_CO2'] = emissions_df[['tons_per_year_RUNEX_CO2', 'tons_per_year_STREX_CO2']].sum(axis=1)\n",
    "\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "    return emissions_df\n",
    "#############################################################\n",
    "################### SAVE FILES FOR INMAP ####################\n",
    "#############################################################\n",
    "\n",
    "def save_emissions(emissions_df, scenario):\n",
    "    start_time = time.time() \n",
    "    \n",
    "    print(' - Function:  save_emissions ...')\n",
    "\n",
    "    #\n",
    "    \n",
    "    global geoAggregationType\n",
    "    \n",
    "    detail_file_name = f'BEAM_INMAP_detail_{geoAggregationType}_{scenario}.csv'\n",
    "    summary_file_name = f'BEAM_INMAP_{geoAggregationType}_{scenario}.csv'\n",
    "    emissions_df.to_csv(detail_file_name)\n",
    "    emissions_df[['ISRM','tons_per_year_ROG','tons_per_year_PM2_5','tons_per_year_SOx','tons_per_year_NOx','tons_per_year_NH3','tons_per_year_CO2']].to_csv(summary_file_name)\n",
    "    emissions_df[emissions_df['mode']=='car'][['ISRM','tons_per_year_ROG','tons_per_year_PM2_5','tons_per_year_SOx','tons_per_year_NOx','tons_per_year_NH3','tons_per_year_CO2']].to_csv(f'BEAM_INMAP_{geoAggregationType}_{scenario}_car.csv')\n",
    "    emissions_df[emissions_df['mode']=='freight'][['ISRM','tons_per_year_ROG','tons_per_year_PM2_5','tons_per_year_SOx','tons_per_year_NOx','tons_per_year_NH3','tons_per_year_CO2']].to_csv(f'BEAM_INMAP_{geoAggregationType}_{scenario}_freight.csv')\n",
    "    emissions_df[emissions_df['mode']=='UBUS'][['ISRM','tons_per_year_ROG','tons_per_year_PM2_5','tons_per_year_SOx','tons_per_year_NOx','tons_per_year_NH3','tons_per_year_CO2']].to_csv(f'BEAM_INMAP_{geoAggregationType}_{scenario}_bus.csv')\n",
    "\n",
    "    print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "#############################################################\n",
    "#############################################################\n",
    "#############################################################\n",
    "#############################################################\n",
    "#############################################################\n",
    "\n",
    "# def main():\n",
    "\n",
    "for scenario, year, iteration, scale, bucket, is_baseline, is_cruise in zip(scenarios, years, iterations, scales, buckets, are_baseline, are_cruise):\n",
    "\n",
    "    abs_start_time = time.time()\n",
    "    #############################################################\n",
    "    print('########################## READ #############################')\n",
    "    #############################################################\n",
    "\n",
    "    print('Read Files ... ')\n",
    "    if is_HaitamTest:\n",
    "        events_fp = f'gs://{bucket}/{scenario}/ITERS/it.0/0.events.csv.gz'\n",
    "        netwrok_fp = 'gs://'+bucket+'/'+scenario+'/network.csv.gz'\n",
    "    else:\n",
    "        events_fp = f'gs://{bucket}/{scenario}/beam/year-{year}-iteration-{iteration}/ITERS/it.0/0.events.csv.gz'\n",
    "        netwrok_fp = 'gs://'+bucket+'/'+scenario+'/beam/year-'+year+'-iteration-'+iteration+'/network.csv.gz'\n",
    "    \n",
    "    # vTypes = pd.read_csv('vehicletypes-baseline.csv',nrows = None)\n",
    "    events = pd.read_csv(\n",
    "        events_fp,\n",
    "        usecols=events_column_types.keys(),\n",
    "        nrows=nrows_events,\n",
    "        dtype=events_column_types)\n",
    "    network = pd.read_csv(netwrok_fp, usecols = ['linkId','fromLocationX','toLocationX','fromLocationY','toLocationY','linkLength'])\n",
    "    network['linkLength'] = network['linkLength'] * correction_factor_curvature\n",
    "\n",
    "    #############################################################\n",
    "    print('########################## NETWORK ##########################')\n",
    "    #############################################################\n",
    "\n",
    "    networkExploded = split_network(network)\n",
    "    networkExploded = calculate_midpoints(networkExploded)\n",
    "    networkExploded = enrich_geo_data(networkExploded, ISRM_grid, BGs, block_info, counties_shapefile)\n",
    "\n",
    "    #############################################################\n",
    "    print('########################## PATHTRAVERSAL ####################')\n",
    "    #############################################################\n",
    "\n",
    "    pathTraversal = filter_events(events)\n",
    "    pathTraversal = drop_na_links(pathTraversal)\n",
    "    pathTraversal = filter_modes(pathTraversal, scenario)\n",
    "    pathTraversal = process_links(pathTraversal)  \n",
    "    pathTraversal = link_times(pathTraversal)\n",
    "\n",
    "    #############################################################\n",
    "    print('########################## NETWORK ##########################')\n",
    "    #############################################################\n",
    "\n",
    "    networkParking = calculate_midpoints(network)\n",
    "    networkParking = enrich_geo_data(networkParking, ISRM_grid, BGs, block_info, counties_shapefile)\n",
    "\n",
    "    #############################################################\n",
    "    print('########################## PARK #############################')\n",
    "    #############################################################\n",
    "\n",
    "    parkDepartDatabase = prepare_park_departDatabase(pathTraversal)\n",
    "    parkingDatabase, departDatabase, parkingDatabase_no_first = prepare_parkingDatabase(parkDepartDatabase, networkParking)\n",
    "\n",
    "    # print(parkingDatabase[parkingDatabase['vehicle']=='rideHailVehicle-663229@Lyft'][['vehicle','parking_time','departureTime']])\n",
    "    # print(parkingDatabase_no_first[parkingDatabase_no_first['vehicle']=='rideHailVehicle-663229@Lyft'][['vehicle','parking_time','departureTime']])\n",
    "    # print(departDatabase[departDatabase['vehicle']=='rideHailVehicle-663229@Lyft'][['vehicle','parking_time','departureTime']])\n",
    "    # print(parkDepartDatabase[parkDepartDatabase['vehicle']=='rideHailVehicle-663229@Lyft'][['vehicle','parking_time','departureTime']])\n",
    "\n",
    "    #############################################################\n",
    "    print('########################## EXPLODE ##########################')\n",
    "    #############################################################\n",
    "\n",
    "    explodedPathTraversal = explode_path_traversal(pathTraversal)\n",
    "    explodedPathTraversal = merge_with_network(explodedPathTraversal, networkExploded)\n",
    "    explodedPathTraversal = adjust_travel_times(explodedPathTraversal)\n",
    "    explodedPathTraversal = calculate_speeds(explodedPathTraversal)\n",
    "\n",
    "    print(len(explodedPathTraversal), '- len of explodedPathTraversal after processing')\n",
    "\n",
    "    #############################################################\n",
    "    print('########################## ASSIGN VEHICLE CLASS #############')\n",
    "    #############################################################\n",
    "\n",
    "    car_EMFACT_vmt = load_EMFACT_data(EMFACT_VMT_filepath, EMFACT_classes_car)\n",
    "    freight_EMFACT_vmt = load_EMFACT_data(EMFACT_VMT_filepath, EMFACT_classes_freight)\n",
    "    car_totalVMTbyCounty = calculate_vmt_distribution(car_EMFACT_vmt)\n",
    "    freight_totalVMTbyCounty = calculate_vmt_distribution(freight_EMFACT_vmt)\n",
    "    car_vmtLookup = create_vmtLookup(car_totalVMTbyCounty)\n",
    "    freight_vmtLookup = create_vmtLookup(freight_totalVMTbyCounty)\n",
    "\n",
    "    explodedPathTraversal = assign_vehicle_classes(explodedPathTraversal, car_vmtLookup, freight_vmtLookup)\n",
    "    parkingDatabase = assign_vehicle_classes(parkingDatabase, car_vmtLookup, freight_vmtLookup)\n",
    "    departDatabase = assign_vehicle_classes(departDatabase, car_vmtLookup, freight_vmtLookup)\n",
    "    parkingDatabase_no_first = assign_vehicle_classes(parkingDatabase_no_first, car_vmtLookup, freight_vmtLookup)\n",
    "    if is_cruise:\n",
    "        explodedPathTraversal = correct_cruise_vehicle_classes(explodedPathTraversal)\n",
    "        parkingDatabase = correct_cruise_vehicle_classes(parkingDatabase)\n",
    "        departDatabase = correct_cruise_vehicle_classes(departDatabase)\n",
    "        parkingDatabase_no_first = correct_cruise_vehicle_classes(parkingDatabase_no_first)\n",
    "    print(len(explodedPathTraversal), len(parkingDatabase), len(departDatabase),  len(parkingDatabase_no_first),)\n",
    "    \n",
    "    explodedPathTraversal_grouped = explodedPathTraversal[[\"vehicle_class_emfact\", \"fuel_type_emfact\",\"linkLength\"]].groupby([\"vehicle_class_emfact\", \"fuel_type_emfact\"]).sum(numeric_only=True).reset_index().to_csv(f'VMT_by_EMFAC_vehicletype_{scenario}.csv')\n",
    "\n",
    "\n",
    "    #############################################################\n",
    "    print('########################## CORRECTION FACTORS ###############')\n",
    "    #############################################################\n",
    "\n",
    "    EMFACT_trips = load_EMFACT_data(EMFACT_trips_filepath, EMFACT_classes)\n",
    "    EMFACT_vmt = load_EMFACT_data(EMFACT_VMT_filepath, EMFACT_classes)\n",
    "\n",
    "    # EMFACT_trips.loc[EMFACT_trips['vehicle_class'].isin(EMFACT_classes_bus) , 'fuel'] = busesFuelType\n",
    "    # EMFACT_vmt.loc[EMFACT_vmt['vehicle_class'].isin(EMFACT_classes_bus) , 'fuel'] = busesFuelType\n",
    "\n",
    "    # EMFACT_trips['vehicle_class'].replace(busVehicleClassDict, inplace=True)\n",
    "    # EMFACT_vmt['vehicle_class'].replace(busVehicleClassDict, inplace=True)\n",
    "\n",
    "    if is_baseline:\n",
    "        corr_VMT_by_county_dict, corr_Trips_by_county_dict = calculate_correction_factors(explodedPathTraversal, departDatabase, EMFACT_vmt, EMFACT_trips, scale, scenario)\n",
    "    explodedPathTraversal = apply_correction_factors(explodedPathTraversal, corr_VMT_by_county_dict, 'corr_VMT_by_county')\n",
    "    parkingDatabase= apply_correction_factors(parkingDatabase, corr_Trips_by_county_dict, 'corr_trips_by_county')\n",
    "    departDatabase= apply_correction_factors(departDatabase, corr_Trips_by_county_dict, 'corr_trips_by_county')\n",
    "    parkingDatabase_no_first = apply_correction_factors(parkingDatabase_no_first, corr_Trips_by_county_dict, 'corr_trips_by_county')\n",
    "\n",
    "    #############################################################\n",
    "    print('########################## EMFACT ###########################')\n",
    "    #############################################################\n",
    "\n",
    "    emfactEmisRates = load_EMFACT_data(EMFACTemfactFilepath, EMFACT_classes)\n",
    "    emfactEmisRates = rename_emfact_columns(emfactEmisRates)\n",
    "    explodedPathTraversal= categorize_speed_data(explodedPathTraversal)\n",
    "    departDatabase = adjust_departDatabase_speed(departDatabase)\n",
    "\n",
    "    #############################################################\n",
    "    print('########################## ASSIGN EMFACT ####################')\n",
    "    #############################################################\n",
    "    explodedPathTraversal, parkingDatabase, departDatabase, parkingDatabase_no_first = process_emission_rates(emfactEmisRates, explodedPathTraversal, parkingDatabase, departDatabase, parkingDatabase_no_first, scale)\n",
    "\n",
    "    #############################################################\n",
    "    print('########################## AGGREGATE ########################')\n",
    "    #############################################################\n",
    "    print(explodedPathTraversal.keys())\n",
    "#         explodedPathTraversal.loc[explodedPathTraversal['bgid'] == 'Other', 'county_name'] = 'Other'\n",
    "    county_counts = (\n",
    "        explodedPathTraversal.groupby(['ISRM', 'county_name'])\n",
    "        .size()\n",
    "        .reset_index(name='county_counts')\n",
    "    )\n",
    "\n",
    "    # Determine the most frequent county_tag for each ISRM\n",
    "    most_frequent_county = (\n",
    "        county_counts.sort_values('county_counts', ascending=False)\n",
    "        .drop_duplicates(subset=['ISRM'])\n",
    "        .sort_values('ISRM')\n",
    "    )\n",
    "    isrmToCountyDict = most_frequent_county.set_index('ISRM')['county_name'].to_dict()\n",
    "#         isrmToCountyDict = explodedPathTraversal[['ISRM', 'county_name']].drop_duplicates().set_index('ISRM')['county_name'].to_dict()\n",
    "\n",
    "    BEAM_emis, BEAM_emis_park, BEAM_emis_park_no_first, BEAM_emis_dep = aggregate_emissions(explodedPathTraversal, parkingDatabase, departDatabase, parkingDatabase_no_first )\n",
    "    BEAM_emis = merge_emission_dataframes([BEAM_emis, BEAM_emis_park, BEAM_emis_park_no_first, BEAM_emis_dep])\n",
    "    BEAM_emis = calculate_total_emissions(BEAM_emis)\n",
    "\n",
    "    BEAM_emis['county_name'] = BEAM_emis['ISRM'].map(isrmToCountyDict)\n",
    "\n",
    "    #############################################################\n",
    "    print('########################## SAVE ########################')\n",
    "    #############################################################\n",
    "\n",
    "    # Saving emission data\n",
    "    save_emissions(BEAM_emis, scenario)\n",
    "\n",
    "    explodedPathTraversal.groupby(['link','segment_part']).agg(\n",
    "        {'linkLength':sum}\n",
    "    ).reset_index().merge(\n",
    "        networkExploded[['linkId','X', 'Y','segment_part']], \n",
    "        left_on = ['link', 'segment_part'], \n",
    "        right_on = ['linkId', 'segment_part']\n",
    "       ).to_csv(f'VMT_{geoAggregationType}_{scenario}.csv')\n",
    "\n",
    "    explodedPathTraversal.groupby(['link','segment_part']).agg(\n",
    "        {'travelTime':sum}\n",
    "    ).reset_index().merge(\n",
    "        networkExploded[['linkId','X', 'Y','segment_part']], \n",
    "        left_on = ['link', 'segment_part'], \n",
    "        right_on = ['linkId', 'segment_part']\n",
    "       ).to_csv(f'VHT_{geoAggregationType}_{scenario}.csv')\n",
    "\n",
    "    explodedPathTraversal.groupby(['link', 'segment_part']).agg(\n",
    "        linkLength_sum=('linkLength', 'sum'),\n",
    "        count=('linkLength', 'size')\n",
    "        ).reset_index().merge(\n",
    "        networkExploded[['linkId', 'X', 'Y', 'segment_part']],\n",
    "        left_on=['link', 'segment_part'],\n",
    "        right_on=['linkId', 'segment_part']\n",
    "        ).to_csv(f'Flow_{geoAggregationType}_{scenario}.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c9c905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "explodedPathTraversal[explodedPathTraversal.PTID == 28338295].to_csv('example.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cba7b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "explodedPathTraversal_grouped = explodedPathTraversal[[\"vehicle_class_emfact\", \"fuel_type_emfact\",\"linkLength\"]].groupby([\"vehicle_class_emfact\", \"fuel_type_emfact\"]).sum(numeric_only=True).reset_index().to_csv(f'VMT_by_EMFAC_vehicletype_{scenario}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3ad48d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vehicle_class_emfact</th>\n",
       "      <th>fuel_type_emfact</th>\n",
       "      <th>linkLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LDA</td>\n",
       "      <td>Dsl</td>\n",
       "      <td>3.434742e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LDA</td>\n",
       "      <td>Elec</td>\n",
       "      <td>2.199952e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LDA</td>\n",
       "      <td>Gas</td>\n",
       "      <td>7.413032e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LDA</td>\n",
       "      <td>Phe</td>\n",
       "      <td>1.143466e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LDT1</td>\n",
       "      <td>Dsl</td>\n",
       "      <td>4.809648e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>T7 Tractor Class 8</td>\n",
       "      <td>Dsl</td>\n",
       "      <td>9.409463e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>T7 Tractor Class 8</td>\n",
       "      <td>NG</td>\n",
       "      <td>6.565134e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>T7 Utility Class 8</td>\n",
       "      <td>Dsl</td>\n",
       "      <td>1.816221e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>T7IS</td>\n",
       "      <td>Gas</td>\n",
       "      <td>2.788191e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>UBUS</td>\n",
       "      <td>Dsl</td>\n",
       "      <td>4.679479e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   vehicle_class_emfact fuel_type_emfact    linkLength\n",
       "0                   LDA              Dsl  3.434742e+04\n",
       "1                   LDA             Elec  2.199952e+05\n",
       "2                   LDA              Gas  7.413032e+06\n",
       "3                   LDA              Phe  1.143466e+05\n",
       "4                  LDT1              Dsl  4.809648e+02\n",
       "..                  ...              ...           ...\n",
       "83   T7 Tractor Class 8              Dsl  9.409463e+04\n",
       "84   T7 Tractor Class 8               NG  6.565134e+03\n",
       "85   T7 Utility Class 8              Dsl  1.816221e+03\n",
       "86                 T7IS              Gas  2.788191e+02\n",
       "87                 UBUS              Dsl  4.679479e+05\n",
       "\n",
       "[88 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explodedPathTraversal_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68f569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    ########################################################################################\n",
    "    ########################################################################################\n",
    "    ################################          PLOT         #################################\n",
    "    ########################################################################################\n",
    "    ########################################################################################\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import r2_score\n",
    "\n",
    "    emis = BEAM_emis.copy()\n",
    "#         emis = emis[emis['county_name'] != 'Other']\n",
    "\n",
    "\n",
    "    emis = emis[['ISRM','tons_per_year_ROG','tons_per_year_NOx',\n",
    "                 'tons_per_year_NH3','tons_per_year_SOx','tons_per_year_PM2_5',\n",
    "                 'tons_per_year_CO2'\n",
    "                ]]\n",
    "\n",
    "    emis = emis.groupby('ISRM').agg({\n",
    "        'tons_per_year_ROG': 'sum',\n",
    "        'tons_per_year_NOx': 'sum',\n",
    "        'tons_per_year_NH3': 'sum',\n",
    "        'tons_per_year_SOx': 'sum',\n",
    "        'tons_per_year_PM2_5': 'sum',\n",
    "        'tons_per_year_CO2': 'sum',\n",
    "    }).reset_index()\n",
    "\n",
    "    emis_shapefile_filepath = '../BEAM_to_EMFACT/isrm_polygon/isrm_polygon.shp'\n",
    "    gdf = gpd.read_file(emis_shapefile_filepath)\n",
    "    emis['ISRM'] = emis['ISRM'].astype(str).str.upper()\n",
    "    gdf['isrm'] = gdf['isrm'].astype(str).str.upper()\n",
    "    emis = emis.merge(gdf[['isrm', 'geometry']], left_on='ISRM', right_on='isrm')\n",
    "    emis = gpd.GeoDataFrame(emis, geometry='geometry')\n",
    "    emis['ISRM'] = emis['ISRM'].astype(int)\n",
    "    emis['area'] = emis.geometry.area\n",
    "    emis['tons_per_year_PM2_5/area_square_meters'] = emis['tons_per_year_PM2_5']/emis['area']\n",
    "    emis.to_file('emis'+scenario+'.shp')\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 15))\n",
    "    emis.plot(column='tons_per_year_PM2_5/area_square_meters', ax=ax, legend=True,\n",
    "             legend_kwds={'label': f\"{'tons_per_year_PM2_5/area_square_meters'} concentration\",\n",
    "                          })\n",
    "    ax.set_title(f\"{'tons_per_year_/area'} Levels\")\n",
    "    plt.savefig(f'Plots/{scenario}_Emissions_Plot.png')\n",
    "\n",
    "    def rename_columns(pivot_table):\n",
    "        pivot_table.columns = ['tons_per_year_' + '_'.join(col).upper() for col in pivot_table.columns.values]\n",
    "        pivot_table.reset_index(inplace=True)\n",
    "        pivot_table.fillna(0, inplace=True)\n",
    "        pivot_table.rename(columns={'tons_per_year_RUNEX_NOX': 'tons_per_year_RUNEX_NOx',\n",
    "                                    'tons_per_year_RUNEX_SOX': 'tons_per_year_RUNEX_SOx',\n",
    "                                    'tons_per_year_STREX_NOX': 'tons_per_year_STREX_NOx',\n",
    "                                    'tons_per_year_STREX_SOX': 'tons_per_year_STREX_SOx',\n",
    "                                    }, inplace=True)\n",
    "        return pivot_table\n",
    "\n",
    "    def plot_per_case(EMFACT_emis_county, BEAM_emis_county, case, scenario):\n",
    "\n",
    "\n",
    "        merged_data = BEAM_emis_county.merge(EMFACT_emis_county, left_on=\"county_name\", right_on=\"sub_area\")\n",
    "\n",
    "        for col in matching_columns[:-5]:\n",
    "            pollutant_name = col[14:].replace('_', '-', 1)\n",
    "            formatted_title = f\"Emissions from Process Comparison: $\\\\bf{{{pollutant_name}}}$\"\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            ########################\n",
    "            plt.bar(merged_data[\"sub_area\"], merged_data[col+\"_x\"], width=0.4, label=\"Simulated \" + col[14:], align='center', color='DeepSkyBlue')\n",
    "            plt.bar(merged_data[\"sub_area\"], merged_data[col+\"_y\"], width=0.4, label=\"Observed \" + col[14:], align='edge', color='Blue')\n",
    "            ########################\n",
    "            plt.title(formatted_title)\n",
    "            plt.ylabel(\"Tons per Day\")\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'Plots/{col}_{case}_{scenario}_Emissions_Comparison.png')\n",
    "\n",
    "    def histogram_per_case(EMFACT_emis_county, BEAM_emis_county, case, scenario, pollutants):\n",
    "\n",
    "\n",
    "        colors = ['blue', 'green', 'red', 'purple', 'orange', 'gray']\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(25, 10))\n",
    "        bar_width = 0.08\n",
    "        space_between_groups = 0\n",
    "        num_counties = len(EMFACT_emis_county.index)\n",
    "        total_bars = len(pollutants) * 2\n",
    "        group_positions = np.arange(num_counties)\n",
    "        for i, pollutant in enumerate(pollutants):\n",
    "            ########################\n",
    "            modelled_data = BEAM_emis_county[pollutant]\n",
    "            observed_data = EMFACT_emis_county[pollutant]\n",
    "            ########################\n",
    "            modelled_positions = group_positions + i * (bar_width * 2 + space_between_groups)\n",
    "            observed_positions = modelled_positions + bar_width\n",
    "            ax.bar(modelled_positions, modelled_data, bar_width, label=f'{pollutant[14:]} Tons per Day Modelled',\n",
    "                   color=colors[i])\n",
    "            ax.bar(observed_positions, observed_data, bar_width, label=f'{pollutant[14:]} Tons per Day Observed',\n",
    "                   color=colors[i], alpha=0.5, edgecolor='black', hatch='//')\n",
    "            ax.set_xlabel('County')\n",
    "        ax.set_ylabel('Tons per Day')\n",
    "        ax.set_title('Total Pollutant Emissions by County: Modelled vs Observed')\n",
    "        ax.set_xticks(group_positions + total_bars / 2 * bar_width)\n",
    "        ax.set_xticklabels(EMFACT_emis_county.index, rotation=45)\n",
    "        ax.legend(title='Pollutants', bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'Plots/All_Emissions_Comparison_{case}_{scenario}.png')\n",
    "\n",
    "#         def regression_per_case(EMFACT_emis_county, BEAM_emis_county, case, scenario):\n",
    "\n",
    "\n",
    "#             global_min = float('inf')\n",
    "#             global_max = float('-inf')\n",
    "#             pollutant_colors = {\n",
    "#                 'tons_per_year_PM2_5': 'blue',\n",
    "#                 'tons_per_year_SOx': 'orange',\n",
    "#                 'tons_per_year_NOx': 'green',\n",
    "#                 'tons_per_year_ROG': 'red',\n",
    "#                 'tons_per_year_NH3': 'purple'\n",
    "#             }\n",
    "#             fig, ax = plt.subplots(figsize=(15, 8))\n",
    "#             for pollutant, color in pollutant_colors.items():\n",
    "#                 ########################\n",
    "#                 modelled_values = BEAM_emis_county[pollutant].values\n",
    "#                 observed_values = EMFACT_emis_county[pollutant].values\n",
    "#                 ########################\n",
    "#                 global_min = min(global_min, modelled_values.min(), observed_values.min())\n",
    "#                 global_max = max(global_max, modelled_values.max(), observed_values.max())\n",
    "#                 ax.scatter(observed_values, modelled_values, color=color, label=f'{pollutant[14:]} Scatter')\n",
    "#                 m, b = np.polyfit(observed_values, modelled_values, 1)\n",
    "#                 y_pred = m * observed_values + b\n",
    "#                 r_squared = r2_score(modelled_values, y_pred)\n",
    "#                 x = np.linspace(observed_values.min(), observed_values.max(), 100)\n",
    "#                 ax.plot(x, m * x + b, color=color, label=f'{pollutant[14:]} Regression Line')\n",
    "#                 equation_text = f'{pollutant[14:]}: y = {m:.2f}x + {b:.2f}, $R^2$ = {r_squared:.2f}'\n",
    "#                 ax.annotate(equation_text, xy=(0.05, 0.95 - 0.05 * list(pollutant_colors.keys()).index(pollutant)),\n",
    "#                             xycoords='axes fraction', ha='left', va='top', fontsize=8, color=color)\n",
    "\n",
    "#             bisector_x = np.linspace(global_min, global_max, 100)\n",
    "#             ax.plot(bisector_x, bisector_x, 'k--', label='1:1 Line')\n",
    "#             ax.set_ylabel('Modelled Emissions (tons per day)')\n",
    "#             ax.set_xlabel('Observed Emissions (tons per day)') \n",
    "#             ax.set_title('Comparison of Observed and Modelled Emissions') \n",
    "#             ax.grid(True)\n",
    "#             ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title='Pollutants')\n",
    "#             plt.tight_layout()\n",
    "#             plt.savefig(f'Plots/All_Emissions_Regression_Inverted_{case}_{scenario}.png')\n",
    "\n",
    "\n",
    "    def regression_per_case(EMFACT_emis_county, BEAM_emis_county, case, scenario):\n",
    "        global_min = float('inf')\n",
    "        global_max = float('-inf')\n",
    "\n",
    "        # More formal color scheme\n",
    "        pollutant_colors = {\n",
    "            'tons_per_year_PM2_5': '#1f77b4',  # blue\n",
    "            'tons_per_year_SOx': '#ff7f0e',    # orange\n",
    "            'tons_per_year_NOx': '#2ca02c',    # green\n",
    "            'tons_per_year_ROG': '#d62728',    # red\n",
    "            'tons_per_year_NH3': '#9467bd',     # purple\n",
    "#             'tons_per_year_CO2': '#bcbd22'     # boh\n",
    "        }\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "        for pollutant, color in pollutant_colors.items():\n",
    "            modelled_values = BEAM_emis_county[pollutant].values\n",
    "            observed_values = EMFACT_emis_county[pollutant].values\n",
    "\n",
    "            global_min = min(global_min, modelled_values.min(), observed_values.min())\n",
    "            global_max = max(global_max, modelled_values.max(), observed_values.max())\n",
    "\n",
    "            ax.scatter(observed_values, modelled_values, color=color, label=f'{pollutant[14:]} per County', s=100)\n",
    "            m, b = np.polyfit(observed_values, modelled_values, 1)\n",
    "            y_pred = m * observed_values + b\n",
    "            r_squared = r2_score(modelled_values, y_pred)\n",
    "\n",
    "            x = np.linspace(observed_values.min(), observed_values.max(), 100)\n",
    "            ax.plot(x, m * x + b, color=color, label=f'{pollutant[14:]} Regr. Line', linewidth=2)\n",
    "\n",
    "            equation_text = f'{pollutant[14:]}: y = {m:.2f}x + {b:.2f}, $R^2$ = {r_squared:.2f}'\n",
    "            ax.annotate(equation_text, \n",
    "                        xy=(0.05, 0.95 - 0.05 * list(pollutant_colors.keys()).index(pollutant)), \n",
    "                        xycoords='axes fraction', ha='left', va='top', fontsize=14, color=color)\n",
    "\n",
    "        bisector_x = np.linspace(global_min, global_max, 100)\n",
    "        ax.plot(bisector_x, bisector_x, 'k--', label='1:1 Line', linewidth=2)\n",
    "\n",
    "        ax.set_ylabel('Modelled Emissions (tons per day)', fontsize=18)\n",
    "        ax.set_xlabel('Observed Emissions (tons per day)', fontsize=18) \n",
    "        ax.set_title('Comparison of Observed and Modelled Emissions', fontsize=20) \n",
    "        ax.grid(True)\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title='Pollutants', fontsize=14)\n",
    "\n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.savefig(f'Plots/All_Emissions_Regression_Inverted_{case}_{scenario}.png')\n",
    "        plt.show()\n",
    "\n",
    "    def histogram_per_pollutant(EMFACT_emis_county, BEAM_emis_county, case, scenario):\n",
    "\n",
    "        colors = ['blue', 'green', 'red', 'purple', 'orange', 'gray']\n",
    "        for i, pollutant in enumerate(pollutants):\n",
    "            fig, ax = plt.subplots(figsize=(10, 5))\n",
    "            ########################\n",
    "            modelled_data = BEAM_emis_county[pollutant]\n",
    "            observed_data = EMFACT_emis_county[pollutant]\n",
    "            ########################\n",
    "            index = np.arange(len(EMFACT_emis_county.index))\n",
    "            bar_width = 0.35\n",
    "            rects1 = ax.bar(index - bar_width / 2, modelled_data, bar_width, label='Modelled', color=colors[i])\n",
    "            rects2 = ax.bar(index + bar_width / 2, observed_data, bar_width, label='Observed', color=colors[i], alpha=0.5,\n",
    "                            edgecolor='black', hatch='//')\n",
    "            ax.set_xlabel('County')\n",
    "            ax.set_ylabel('Tons per Day')\n",
    "            ax.set_title(f'{pollutant[14:]} Emissions by County: Modelled vs Observed')\n",
    "            ax.set_xticks(index)\n",
    "            ax.set_xticklabels(EMFACT_emis_county.index, rotation=45)\n",
    "            ax.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'Plots/validation_{case}_{pollutant}_{scenario}.png')\n",
    "\n",
    "    def compare_VMT(explodedPathTraversal, EMFACT_vmt, case, scenario):\n",
    "\n",
    "\n",
    "        modelled_linkLength_sum = explodedPathTraversal.groupby(['county_name'])['correct_length_bus'].sum()\n",
    "        observed_linkLength_sum = EMFACT_vmt.groupby('sub_area')['total_vmt'].sum()\n",
    "\n",
    "        ########################\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        index = np.arange(len(observed_linkLength_sum.index))\n",
    "        bar_width = 0.35\n",
    "        rects1 = ax.bar(index - bar_width / 2, modelled_linkLength_sum, bar_width, label='Modelled', color='skyblue')\n",
    "        rects2 = ax.bar(index + bar_width / 2, observed_linkLength_sum, bar_width, label='Observed', color='lightgreen')\n",
    "        ax.set_xlabel('County')\n",
    "        ax.set_ylabel('VMT')\n",
    "        ax.set_title('VMT by County: Modelled vs Observed')\n",
    "        ax.set_xticks(index)\n",
    "        ax.set_xticklabels(observed_linkLength_sum.index, rotation=45)\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'Plots/VMT_Comparison_{case}_{scenario}.png')\n",
    "\n",
    "    def aggregate_emissions_plot(explodedPathTraversal, parkingDatabase, departDatabase, parkingDatabase_no_first, EMFACT_classes):\n",
    "        start_time = time.time()\n",
    "        def aggregate_df_emissions_by_isrm_plot(dataframe, processes, pollutants_list):\n",
    "\n",
    "            print(' - Function:  aggregate_emissions_by_isrm ...')\n",
    "\n",
    "            # Aggregate emissions by ISRM\n",
    "\n",
    "            emissions_aggregated = dataframe.groupby('ISRM')[\n",
    "                [f'tons_per_year_{process}_{pollutant}' \n",
    "                 for pollutants, process in zip(pollutants_list, processes)\n",
    "                 for pollutant in pollutants \n",
    "                ]\n",
    "            ].sum().reset_index()\n",
    "            return emissions_aggregated\n",
    "\n",
    "        BEAM_emis = aggregate_df_emissions_by_isrm_plot(explodedPathTraversal[explodedPathTraversal['vehicle_class_emfact'].isin(EMFACT_classes)], \n",
    "                                            ['RUNEX', 'PMBW', 'PMTW', 'RUNLOSS'], \n",
    "                                            [['PM2_5', 'SOx', 'NOx', 'ROG', 'NH3', 'CO2'], ['PM2_5'], ['PM2_5'], ['ROG']])\n",
    "        BEAM_emis_park_no_first = aggregate_df_emissions_by_isrm_plot(parkingDatabase_no_first[parkingDatabase_no_first['vehicle_class_emfact'].isin(EMFACT_classes)], \n",
    "                                                              ['HOTSOAK'], \n",
    "                                                              [['ROG']])\n",
    "        BEAM_emis_park = aggregate_df_emissions_by_isrm_plot(parkingDatabase[parkingDatabase['vehicle_class_emfact'].isin(EMFACT_classes)], \n",
    "                                                     ['DIURN'], \n",
    "                                                     [['ROG']])\n",
    "        BEAM_emis_dep = aggregate_df_emissions_by_isrm_plot(departDatabase[departDatabase['vehicle_class_emfact'].isin(EMFACT_classes)], \n",
    "                                                    ['STREX'], \n",
    "                                                    [['PM2_5', 'SOx', 'NOx', 'ROG', 'CO2']])\n",
    "\n",
    "        print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "        return BEAM_emis, BEAM_emis_park, BEAM_emis_park_no_first, BEAM_emis_dep\n",
    "    \n",
    "    def merge_emission_dataframes_plot(df_list):\n",
    "        start_time = time.time() \n",
    "\n",
    "        print(' - Function:  merge_emission_dataframes ...')\n",
    "\n",
    "        # Merge emissions from different dataframes\n",
    "\n",
    "        result = df_list[0]\n",
    "        for df in df_list[1:]:\n",
    "            result = pd.merge(result, df, on=['ISRM'], how='left')\n",
    "\n",
    "        print(' - Time taken: {:.2f} seconds'.format(time.time() - start_time))\n",
    "        return result\n",
    "\n",
    "    \n",
    "    \n",
    "    pollutants = ['tons_per_year_PM2_5', 'tons_per_year_SOx', 'tons_per_year_NOx',\n",
    "                  'tons_per_year_ROG', 'tons_per_year_NH3']\n",
    "    \n",
    "    # EMFACT EMIS\n",
    "    EMFACT_vmt = load_EMFACT_data(EMFACT_VMT_filepath, EMFACT_classes)\n",
    "    EMFACT_vmt.loc[EMFACT_vmt['vehicle_class'].isin(EMFACT_classes_bus) , 'fuel'] = busesFuelType\n",
    "#     EMFACT_vmt['vehicle_class'].replace(busVehicleClassDict, inplace=True)\n",
    "    EMFACT_vmt.loc[:, 'vehicle_class'].replace(busVehicleClassDict, inplace=True)\n",
    "\n",
    "    EMFACT_emis = load_EMFACT_data(EMFACTemissionsFilepath, EMFACT_classes)\n",
    "    EMFACT_emis_car = EMFACT_emis[EMFACT_emis['vehicle_class'].isin(EMFACT_classes_car)]\n",
    "    EMFACT_emis_bus = EMFACT_emis[EMFACT_emis['vehicle_class'].isin(EMFACT_classes_bus)]\n",
    "    EMFACT_emis_freight = EMFACT_emis[EMFACT_emis['vehicle_class'].isin(EMFACT_classes_freight)]\n",
    "\n",
    "    EMFACT_emis.loc[EMFACT_emis['vehicle_class'].isin(EMFACT_classes_bus) , 'fuel'] = busesFuelType\n",
    "    EMFACT_emis_bus.loc[EMFACT_emis_bus['vehicle_class'].isin(EMFACT_classes_bus) , 'fuel'] = busesFuelType\n",
    "\n",
    "#     EMFACT_emis['vehicle_class'].replace(busVehicleClassDict, inplace=True)\n",
    "    EMFACT_emis.loc[:, 'vehicle_class'].replace(busVehicleClassDict, inplace=True)\n",
    "#     EMFACT_emis_bus['vehicle_class'].replace(busVehicleClassDict, inplace=True)\n",
    "    EMFACT_emis_bus.loc[:, 'vehicle_class'].replace(busVehicleClassDict, inplace=True)\n",
    "\n",
    "\n",
    "    pivot_EMFACT_emis = EMFACT_emis.pivot_table(index='sub_area', columns=['process', 'pollutant'], values='emission', aggfunc='sum')\n",
    "    pivot_EMFACT_emis = rename_columns(pivot_EMFACT_emis)\n",
    "    pivot_EMFACT_emis_car = EMFACT_emis_car.pivot_table(index='sub_area', columns=['process', 'pollutant'], values='emission', aggfunc='sum')\n",
    "    pivot_EMFACT_emis_car = rename_columns(pivot_EMFACT_emis_car)\n",
    "    pivot_EMFACT_emis_bus = EMFACT_emis_bus.pivot_table(index='sub_area', columns=['process', 'pollutant'], values='emission', aggfunc='sum')\n",
    "    pivot_EMFACT_emis_bus = rename_columns(pivot_EMFACT_emis_bus)\n",
    "    pivot_EMFACT_emis_freight = EMFACT_emis_freight.pivot_table(index='sub_area', columns=['process', 'pollutant'], values='emission', aggfunc='sum')\n",
    "    pivot_EMFACT_emis_freight = rename_columns(pivot_EMFACT_emis_freight)\n",
    "\n",
    "    EMFACT_emis_county = calculate_total_emissions_noCO2(pivot_EMFACT_emis)\n",
    "    EMFACT_emis_county_car = calculate_total_emissions_noCO2(pivot_EMFACT_emis_car)\n",
    "    EMFACT_emis_county_bus = calculate_total_emissions_noCO2(pivot_EMFACT_emis_bus)\n",
    "    EMFACT_emis_county_freight = calculate_total_emissions_noCO2(pivot_EMFACT_emis_freight)\n",
    "\n",
    "    matching_columns = ['tons_per_year_RUNEX_PM2_5', 'tons_per_year_RUNEX_SOx', 'tons_per_year_RUNEX_NOx','tons_per_year_RUNEX_ROG', 'tons_per_year_RUNEX_NH3',\n",
    "                        'tons_per_year_PMBW_PM2_5', 'tons_per_year_PMTW_PM2_5', 'tons_per_year_RUNLOSS_ROG', 'tons_per_year_DIURN_ROG', 'tons_per_year_HOTSOAK_ROG',\n",
    "                        'tons_per_year_STREX_PM2_5', 'tons_per_year_STREX_SOx', 'tons_per_year_STREX_NOx',  'tons_per_year_STREX_ROG', 'tons_per_year_ROG', 'tons_per_year_PM2_5',\n",
    "                        'tons_per_year_SOx', 'tons_per_year_NOx', 'tons_per_year_NH3',\n",
    "#                         'tons_per_year_CO2', 'tons_per_year_RUNEX_CO2', 'tons_per_year_STREX_CO2',\n",
    "                       ]\n",
    "    # BEAM EMIS\n",
    "    BEAM_emis_car, BEAM_emis_park_car, BEAM_emis_park_no_first_car, BEAM_emis_dep_car = aggregate_emissions_plot(explodedPathTraversal, parkingDatabase, departDatabase, parkingDatabase_no_first, EMFACT_classes_car )\n",
    "    BEAM_emis_bus, BEAM_emis_park_bus, BEAM_emis_park_no_first_bus, BEAM_emis_dep_bus = aggregate_emissions_plot(explodedPathTraversal, parkingDatabase, departDatabase, parkingDatabase_no_first, EMFACT_classes_bus )\n",
    "    BEAM_emis_freight, BEAM_emis_park_freight, BEAM_emis_park_no_first_freight, BEAM_emis_dep_freight = aggregate_emissions_plot(explodedPathTraversal, parkingDatabase, departDatabase, parkingDatabase_no_first, EMFACT_classes_freight )\n",
    "\n",
    "    BEAM_emis_car = merge_emission_dataframes_plot([BEAM_emis_car, BEAM_emis_park_car, BEAM_emis_park_no_first_car, BEAM_emis_dep_car])\n",
    "    BEAM_emis_bus = merge_emission_dataframes_plot([BEAM_emis_bus, BEAM_emis_park_bus, BEAM_emis_park_no_first_bus, BEAM_emis_dep_bus])\n",
    "    BEAM_emis_freight = merge_emission_dataframes_plot([BEAM_emis_freight, BEAM_emis_park_freight, BEAM_emis_park_no_first_freight, BEAM_emis_dep_freight])\n",
    "\n",
    "    BEAM_emis_car = calculate_total_emissions(BEAM_emis_car)\n",
    "    BEAM_emis_bus = calculate_total_emissions(BEAM_emis_bus)\n",
    "    BEAM_emis_freight = calculate_total_emissions(BEAM_emis_freight)\n",
    "\n",
    "    BEAM_emis_car['county_name'] = BEAM_emis_car['ISRM'].map(isrmToCountyDict)\n",
    "    BEAM_emis_bus['county_name'] = BEAM_emis_bus['ISRM'].map(isrmToCountyDict)\n",
    "    BEAM_emis_freight['county_name'] = BEAM_emis_freight['ISRM'].map(isrmToCountyDict)\n",
    "\n",
    "    BEAM_emis_county = BEAM_emis.groupby('county_name').sum() / days\n",
    "    BEAM_emis_county_car = BEAM_emis_car.groupby('county_name').sum() / days\n",
    "    BEAM_emis_county_bus = BEAM_emis_bus.groupby('county_name').sum() / days\n",
    "    BEAM_emis_county_freight = BEAM_emis_freight.groupby('county_name').sum() / days\n",
    "\n",
    "    merged_data = BEAM_emis_county.merge(pivot_EMFACT_emis, left_on=\"county_name\", right_on=\"sub_area\")\n",
    "    merged_data_car = BEAM_emis_county_car.merge(pivot_EMFACT_emis_car, left_on=\"county_name\", right_on=\"sub_area\")\n",
    "    merged_data_bus = BEAM_emis_county_bus.merge(pivot_EMFACT_emis_bus, left_on=\"county_name\", right_on=\"sub_area\")\n",
    "    merged_data_freight = BEAM_emis_county_freight.merge(pivot_EMFACT_emis_freight, left_on=\"county_name\", right_on=\"sub_area\")\n",
    "\n",
    "    explodedPathTraversal['correct_length'] = explodedPathTraversal['linkLength'] / explodedPathTraversal['corr_VMT_by_county'] / scale                       \n",
    "    explodedPathTraversal['correct_length_bus'] = np.where(\n",
    "        explodedPathTraversal['vehicle_class_emfact'].isin(EMFACT_classes_bus), \n",
    "        explodedPathTraversal['correct_length'] * scale,          \n",
    "        explodedPathTraversal['correct_length']                    \n",
    "    )\n",
    "\n",
    "#         print(EMFACT_emis_county)\n",
    "#         print(BEAM_emis_county)\n",
    "    # PLOT PER CASE\n",
    "    plot_per_case(EMFACT_emis_county, BEAM_emis_county, 'all', scenario)\n",
    "    # HISTOGRAM PER CASE\n",
    "    histogram_per_case(EMFACT_emis_county, BEAM_emis_county, 'all', scenario, pollutants)\n",
    "    # REGRESSION PER CASE\n",
    "    regression_per_case(EMFACT_emis_county, BEAM_emis_county, 'all', scenario)\n",
    "    # HISTOGRAM PER POLLUTANT\n",
    "    histogram_per_pollutant(EMFACT_emis_county, BEAM_emis_county, 'all', scenario)\n",
    "    # COMPARE VMT\n",
    "    compare_VMT(explodedPathTraversal, EMFACT_vmt, 'all', scenario)\n",
    "\n",
    "    # PLOT PER CASE\n",
    "    plot_per_case(EMFACT_emis_county_car, BEAM_emis_county_car, 'car', scenario)\n",
    "    # HISTOGRAM PER CASE\n",
    "    histogram_per_case(EMFACT_emis_county_car, BEAM_emis_county_car, 'car', scenario, pollutants)\n",
    "    # REGRESSION PER CASE\n",
    "    regression_per_case(EMFACT_emis_county_car, BEAM_emis_county_car, 'car', scenario)\n",
    "    # HISTOGRAM PER POLLUTANT\n",
    "    histogram_per_pollutant(EMFACT_emis_county_car, BEAM_emis_county_car, 'car', scenario)\n",
    "    # COMPARE VMT\n",
    "    compare_VMT(explodedPathTraversal[(explodedPathTraversal['mode'] == 'car')], EMFACT_vmt[EMFACT_vmt.vehicle_class.isin(EMFACT_classes_car)], 'car', scenario)\n",
    "\n",
    "    # PLOT PER CASE\n",
    "    plot_per_case(EMFACT_emis_county_bus, BEAM_emis_county_bus, 'bus', scenario)\n",
    "    # HISTOGRAM PER CASE \n",
    "    histogram_per_case(EMFACT_emis_county_bus, BEAM_emis_county_bus, 'bus', scenario, pollutants)\n",
    "    # REGRESSION PER CASE\n",
    "    regression_per_case(EMFACT_emis_county_bus, BEAM_emis_county_bus, 'bus', scenario)\n",
    "    # HISTOGRAM PER POLLUTANT\n",
    "    histogram_per_pollutant(EMFACT_emis_county_bus, BEAM_emis_county_bus, 'bus', scenario)\n",
    "    # COMPARE VMT\n",
    "    compare_VMT(explodedPathTraversal[(explodedPathTraversal['mode'].isin(EMFACT_classes_bus))], EMFACT_vmt[EMFACT_vmt.vehicle_class.isin(EMFACT_classes_bus)], 'bus', scenario)\n",
    "\n",
    "    # PLOT PER CASE\n",
    "    plot_per_case(EMFACT_emis_county_freight, BEAM_emis_county_freight, 'freight', scenario)\n",
    "    # HISTOGRAM PER CASE \n",
    "    histogram_per_case(EMFACT_emis_county_freight, BEAM_emis_county_freight, 'freight', scenario, pollutants)\n",
    "    # REGRESSION PER CASE\n",
    "    regression_per_case(EMFACT_emis_county_freight, BEAM_emis_county_freight, 'freight', scenario)\n",
    "    # HISTOGRAM PER POLLUTANT\n",
    "    histogram_per_pollutant(EMFACT_emis_county_freight, BEAM_emis_county_freight, 'freight', scenario)\n",
    "    # COMPARE VMT\n",
    "    compare_VMT(explodedPathTraversal[(explodedPathTraversal['mode'].isin(EMFACT_classes_freight))], EMFACT_vmt[EMFACT_vmt.vehicle_class.isin(EMFACT_classes_freight)], 'freight', scenario)\n",
    "\n",
    "    print(' - Total Time taken: {:.2f} seconds'.format(time.time() - abs_start_time))\n",
    "\n",
    "\n",
    "del explodedPathTraversal \n",
    "del parkingDatabase \n",
    "del departDatabase\n",
    "del parkingDatabase_no_first\n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef31285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_emis(emis, scenario, scenario2, emissionType, detail_net, is_zoom = False):\n",
    "    print('plot_emis ...')\n",
    "    emis = emis.to_crs(epsg=3857)\n",
    "    detail_net = detail_net.to_crs(epsg=3857)\n",
    "    emis['area'] = emis.geometry.area\n",
    "    emis['tons_per_year_PM2_5/area_square_meters'] = emis['tons_per_year_PM2_5'] / emis['area'] * 2589988.11\n",
    "    emis.to_file(f'{scenario2}_{scenario}_{emissionType}_delta_emis.shp')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "    detail_net.plot(ax=ax, color='grey', alpha=0.05)\n",
    "\n",
    "    ctx.add_basemap(ax, crs=emis.crs.to_string(), source=ctx.providers.OpenStreetMap.Mapnik, alpha=0.65)\n",
    "    cmap = custom_colormap()\n",
    "    \n",
    "    emis[(emis['tons_per_year_PM2_5/area_square_meters'] > 0.001) | (emis['tons_per_year_PM2_5/area_square_meters'] < -0.001)].plot(\n",
    "        ax=ax, column='tons_per_year_PM2_5/area_square_meters', cmap=cmap, legend=True,\n",
    "        legend_kwds={'label': \"PM$_{2.5}$ Delta Emission (Tons per Year per Square Mile)\", 'orientation': \"vertical\"},\n",
    "#         vmin=-max(abs(emis['tons_per_year_PM2_5/area_square_meters'])),vmax=max(abs(emis['tons_per_year_PM2_5/area_square_meters'])), \n",
    "        vmin=-0.2,vmax=0.2, \n",
    "        alpha=0.65\n",
    "    )\n",
    "\n",
    "    if is_zoom:\n",
    "        \n",
    "        ax.set_xlim(-13642750, -13592000)\n",
    "        ax.set_ylim(4527000, 4565000)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        ax.set_xlim(-13662750, -13552000)\n",
    "        ax.set_ylim(4465000, 4585000)\n",
    "        \n",
    "    scalebar = ScaleBar(1, location='lower right', box_color='white', box_alpha=1, color='black', scale_loc='top')\n",
    "    ax.add_artist(scalebar)\n",
    "    north_arrow = FancyArrowPatch((0.1, 0.85), (0.1, 0.95), facecolor='black', edgecolor='black', transform=ax.transAxes, arrowstyle='-|>', mutation_scale=20)\n",
    "    ax.add_patch(north_arrow)\n",
    "    ax.text(0.1, 0.95, 'N', transform=ax.transAxes, fontsize=20, ha='center', va='bottom')\n",
    "    ax.axis('off')\n",
    "    plt.savefig(f'{scenario2}_{scenario}_{emissionType}_{is_zoom}_PM25EmissionMap.png', dpi=600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f4298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathTraversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ced5023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e3a445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190ba613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8857b4de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4ca10e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dae4110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff679d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd6f3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "explodedPathTraversal.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97da9745",
   "metadata": {},
   "outputs": [],
   "source": [
    "explodedPathTraversal[:30].to_csv('tet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7109cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "explodedPathTraversal[['trip_duration',]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b441dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "explodedPathTraversal.trip_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e80451",
   "metadata": {},
   "outputs": [],
   "source": [
    "explodedPathTraversal.travelTime.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39934411",
   "metadata": {},
   "outputs": [],
   "source": [
    "explodedPathTraversal.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b850df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "explodedPathTraversal.linkLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43351924",
   "metadata": {},
   "outputs": [],
   "source": [
    "explodedPathTraversal.correct_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71482cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "explodedPathTraversal.correct_length_bus.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e617ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################################################################################\n",
    "# ########################################################################################\n",
    "# ################################          PLOT         #################################\n",
    "# ########################################################################################\n",
    "# ########################################################################################\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import r2_score\n",
    "\n",
    "# scenario = 'sfbay-tr_capacity_1_5-20230608'\n",
    "\n",
    "# BEAM_emis = pd.read_csv('BEAM_INMAP_detail_ISRM_sfbay-tr-discount-50-20230703.csv')\n",
    "\n",
    "\n",
    "\n",
    "# def regression_per_case(EMFACT_emis_county, BEAM_emis_county, case, scenario):\n",
    "#     global_min = float('inf')\n",
    "#     global_max = float('-inf')\n",
    "    \n",
    "#     # More formal color scheme\n",
    "#     pollutant_colors = {\n",
    "#         'tons_per_year_PM2_5': '#1f77b4',  # blue\n",
    "#         'tons_per_year_SOx': '#ff7f0e',    # orange\n",
    "#         'tons_per_year_NOx': '#2ca02c',    # green\n",
    "#         'tons_per_year_ROG': '#d62728',    # red\n",
    "#         'tons_per_year_NH3': '#9467bd'     # purple\n",
    "#     }\n",
    "    \n",
    "#     fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    \n",
    "#     for pollutant, color in pollutant_colors.items():\n",
    "#         modelled_values = BEAM_emis_county[pollutant].values\n",
    "#         observed_values = EMFACT_emis_county[pollutant].values\n",
    "        \n",
    "#         global_min = min(global_min, modelled_values.min(), observed_values.min())\n",
    "#         global_max = max(global_max, modelled_values.max(), observed_values.max())\n",
    "        \n",
    "#         ax.scatter(observed_values, modelled_values, color=color, label=f'{pollutant[14:]} per County', s=100)\n",
    "#         m, b = np.polyfit(observed_values, modelled_values, 1)\n",
    "#         y_pred = m * observed_values + b\n",
    "#         r_squared = r2_score(modelled_values, y_pred)\n",
    "        \n",
    "#         x = np.linspace(observed_values.min(), observed_values.max(), 100)\n",
    "#         ax.plot(x, m * x + b, color=color, label=f'{pollutant[14:]} Regr. Line', linewidth=2)\n",
    "        \n",
    "#         equation_text = f'{pollutant[14:]}: y = {m:.2f}x + {b:.2f}, $R^2$ = {r_squared:.2f}'\n",
    "#         ax.annotate(equation_text, \n",
    "#                     xy=(0.05, 0.95 - 0.05 * list(pollutant_colors.keys()).index(pollutant)), \n",
    "#                     xycoords='axes fraction', ha='left', va='top', fontsize=14, color=color)\n",
    "\n",
    "#     bisector_x = np.linspace(global_min, global_max, 100)\n",
    "#     ax.plot(bisector_x, bisector_x, 'k--', label='1:1 Line', linewidth=2)\n",
    "    \n",
    "#     ax.set_ylabel('Modelled Emissions (tons per day)', fontsize=18)\n",
    "#     ax.set_xlabel('Observed Emissions (tons per day)', fontsize=18) \n",
    "#     ax.set_title('Comparison of Observed and Modelled Emissions', fontsize=20) \n",
    "#     ax.grid(True)\n",
    "#     ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title='Pollutants', fontsize=14)\n",
    "    \n",
    "#     plt.xticks(fontsize=14)\n",
    "#     plt.yticks(fontsize=14)\n",
    "#     plt.tight_layout()\n",
    "    \n",
    "#     plt.savefig(f'Plots/All_Emissions_Regression_Inverted_{case}_{scenario}.png')\n",
    "#     plt.show()\n",
    "\n",
    "# ################################################################\n",
    "\n",
    "# pollutants = ['tons_per_year_PM2_5', 'tons_per_year_SOx', 'tons_per_year_NOx',\n",
    "#                   'tons_per_year_ROG', 'tons_per_year_NH3']\n",
    "\n",
    "# # EMFACT EMIS\n",
    "# EMFACT_vmt = load_EMFACT_data(EMFACT_VMT_filepath, EMFACT_classes)\n",
    "# EMFACT_vmt.loc[EMFACT_vmt['vehicle_class'].isin(EMFACT_classes_bus) , 'fuel'] = busesFuelType\n",
    "# #     EMFACT_vmt['vehicle_class'].replace(busVehicleClassDict, inplace=True)\n",
    "# EMFACT_vmt.loc[:, 'vehicle_class'].replace(busVehicleClassDict, inplace=True)\n",
    "\n",
    "# EMFACT_emis = load_EMFACT_data(EMFACTemissionsFilepath, EMFACT_classes)\n",
    "# EMFACT_emis_car = EMFACT_emis[EMFACT_emis['vehicle_class'].isin(EMFACT_classes_car)]\n",
    "# EMFACT_emis_bus = EMFACT_emis[EMFACT_emis['vehicle_class'].isin(EMFACT_classes_bus)]\n",
    "\n",
    "# EMFACT_emis.loc[EMFACT_emis['vehicle_class'].isin(EMFACT_classes_bus) , 'fuel'] = busesFuelType\n",
    "# EMFACT_emis_bus.loc[EMFACT_emis_bus['vehicle_class'].isin(EMFACT_classes_bus) , 'fuel'] = busesFuelType\n",
    "\n",
    "# #     EMFACT_emis['vehicle_class'].replace(busVehicleClassDict, inplace=True)\n",
    "# EMFACT_emis.loc[:, 'vehicle_class'].replace(busVehicleClassDict, inplace=True)\n",
    "# #     EMFACT_emis_bus['vehicle_class'].replace(busVehicleClassDict, inplace=True)\n",
    "# EMFACT_emis_bus.loc[:, 'vehicle_class'].replace(busVehicleClassDict, inplace=True)\n",
    "\n",
    "\n",
    "# pivot_EMFACT_emis = EMFACT_emis.pivot_table(index='sub_area', columns=['process', 'pollutant'], values='emission', aggfunc='sum')\n",
    "# pivot_EMFACT_emis = rename_columns(pivot_EMFACT_emis)\n",
    "\n",
    "# EMFACT_emis_county = calculate_total_emissions(pivot_EMFACT_emis)\n",
    "# EMFACT_emis_county_car = calculate_total_emissions(pivot_EMFACT_emis_car)\n",
    "# EMFACT_emis_county_bus = calculate_total_emissions(pivot_EMFACT_emis_bus)\n",
    "\n",
    "# matching_columns = ['tons_per_year_RUNEX_PM2_5', 'tons_per_year_RUNEX_SOx', 'tons_per_year_RUNEX_NOx', 'tons_per_year_RUNEX_ROG', 'tons_per_year_RUNEX_NH3',\n",
    "#                     'tons_per_year_PMBW_PM2_5', 'tons_per_year_PMTW_PM2_5', 'tons_per_year_RUNLOSS_ROG', 'tons_per_year_DIURN_ROG', 'tons_per_year_HOTSOAK_ROG',\n",
    "#                     'tons_per_year_STREX_PM2_5', 'tons_per_year_STREX_SOx', 'tons_per_year_STREX_NOx', 'tons_per_year_STREX_ROG', 'tons_per_year_ROG', 'tons_per_year_PM2_5',\n",
    "#                     'tons_per_year_SOx', 'tons_per_year_NOx', 'tons_per_year_NH3']\n",
    "\n",
    "# BEAM_emis_county = BEAM_emis.groupby('county_name').sum() / days\n",
    "\n",
    "# merged_data = BEAM_emis_county.merge(pivot_EMFACT_emis, left_on=\"county_name\", right_on=\"sub_area\")\n",
    "\n",
    "\n",
    "# regression_per_case(EMFACT_emis_county, BEAM_emis_county, 'all', scenario)\n",
    "\n",
    "# print(' - Total Time taken: {:.2f} seconds'.format(time.time() - abs_start_time))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed70a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f492d1a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb73d92c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860de791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################Compare BEAM Transit VMT with EMFACT one\n",
    "\n",
    "# EMFACT_vmt_bus_comparison = load_EMFACT_data(EMFACT_VMT_filepath, ['UBUS',\n",
    "#                                                     'All Other Buses',\n",
    "#                                                     'OBUS',\n",
    "#                                                     'SBUS'\n",
    "#                                                    ])\n",
    "# EMFACT_vmt_bus_comparison['fuel']=='Dsl'\n",
    "# totalVMTbyCounty_comparison = EMFACT_vmt_bus_comparison.groupby(['sub_area','vehicle_class']).agg({'total_vmt': 'sum'})\n",
    "\n",
    "# explodedPathTraversal_bus_comparison = explodedPathTraversal[explodedPathTraversal['mode'].isin(EMFACT_classes_bus)]\n",
    "# totalVMTbyCounty_BEAM_comparison = explodedPathTraversal_bus_comparison.groupby(['county_name']).agg({'linkLength': 'sum'})\n",
    "\n",
    "# totalVMTbyCounty_BEAM_comparison['county_name2']=totalVMTbyCounty_BEAM_comparison.index\n",
    "# totalVMTbyCounty_BEAM_comparison.reset_index(inplace=True)\n",
    "\n",
    "# import seaborn as sns\n",
    "\n",
    "# emfac_agg = totalVMTbyCounty_comparison.pivot_table(index='sub_area', columns='vehicle_class', values='total_vmt', aggfunc='sum')\n",
    "# class_order = ['UBUS', 'OBUS', 'All Other Buses', 'SBUS']\n",
    "# emfac_agg = emfac_agg.reindex(columns=class_order)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(12, 6))\n",
    "# sns.set_palette(\"Set2\")\n",
    "# palette = sns.color_palette(\"Set2\", n_colors=len(class_order))\n",
    "# emfac_agg.plot(kind='bar', stacked=True, ax=ax, position=0, width=0.4, label='EMFAC', color = palette)\n",
    "# totalVMTbyCounty_BEAM_comparison.set_index('county_name')['linkLength'].plot(kind='bar', ax=ax, position=1, width=0.4, color='seagreen', label='BEAM')\n",
    "# ax.set_ylabel('Total VMT')\n",
    "# ax.set_xlabel('County Name')\n",
    "# ax.set_title('Transit VMT Comparison: EMFAC vs BEAM')\n",
    "# ax.legend(title='Categories')\n",
    "# plt.xticks(rotation=0)\n",
    "# plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "# plt.xticks(rotation=45, ha='right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d0221",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################Correct length factor for straight links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593ef7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correction_factor_curvature = 1.00\n",
    "# import geopandas as gpd\n",
    "# import pandas as pd\n",
    "# from shapely.geometry import LineString\n",
    "\n",
    "# # Adjust this path to your shapefile\n",
    "# filepath_detailed_network = 'sfbay-unclassified-unsimplified-unprojected.osm.shp/lines.shp'\n",
    "\n",
    "# # Load only the required columns into a GeoDataFrame\n",
    "# required_columns = ['geometry', 'other_tags']\n",
    "# network_detailed = gpd.read_file(filepath_detailed_network)\n",
    "\n",
    "# # Function to extract the first and last points from a LineString geometry\n",
    "# def get_first_and_last_points(geometry):\n",
    "#     if isinstance(geometry, LineString):\n",
    "#         return geometry.coords[0], geometry.coords[-1]\n",
    "#     return None, None\n",
    "\n",
    "# # Extract first and last points\n",
    "# network_detailed['first_point'], network_detailed['last_point'] = zip(*network_detailed['geometry'].map(get_first_and_last_points))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb9e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Updated function to safely extract 'osm_id' from 'other_tags'\n",
    "# def extract_osm_id(tags):\n",
    "#     try:\n",
    "#         if tags and 'osmid' in tags:\n",
    "#             return str(tags).split('\"osmid\"=>')[1].split('\"')[1]\n",
    "#     except IndexError:\n",
    "#         return None\n",
    "\n",
    "# # Updated function to safely extract 'lanes' from 'other_tags'\n",
    "# def extract_lanes(tags):\n",
    "#     try:\n",
    "#         if tags and 'lanes' in tags:\n",
    "#             return str(tags).split('\"lanes\"=>')[1].split('\"')[1]\n",
    "#     except IndexError:\n",
    "#         return None\n",
    "    \n",
    "# # Updated function to safely extract 'lanes' from 'other_tags'\n",
    "# def extract_length(tags):\n",
    "#     try:\n",
    "#         if tags and 'length' in tags:\n",
    "#             return str(tags).split('\"length\"=>')[1].split('\"')[1]\n",
    "#     except IndexError:\n",
    "#         return None\n",
    "\n",
    "# # Apply functions to extract attributes from 'other_tags'\n",
    "# network_detailed['osm_id'] = network_detailed['other_tags'].map(extract_osm_id)\n",
    "# network_detailed['lanes'] = network_detailed['other_tags'].map(extract_lanes)\n",
    "# network_detailed['length'] = network_detailed['other_tags'].map(extract_length)\n",
    "# network_detailed['osm_id'] = network_detailed['osm_id'].astype(int)\n",
    "# network_detailed['lanes'] = network_detailed['lanes'].astype(int)\n",
    "# network_detailed['length'] = network_detailed['length'].astype(float)\n",
    "# # Keep only the required columns\n",
    "# final_data = network_detailed[['osm_id', 'lanes', 'first_point', 'last_point', 'length']]\n",
    "\n",
    "# # Display the filtered DataFrame\n",
    "# print(final_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a1b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from shapely.geometry import LineString, Point\n",
    "# from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "# def calculate_direct_distance(point1, point2):\n",
    "#     # Convert degrees to radians\n",
    "#     lat1, lon1 = radians(point1[1]), radians(point1[0])\n",
    "#     lat2, lon2 = radians(point2[1]), radians(point2[0])\n",
    "    \n",
    "#     # Haversine formula\n",
    "#     dlat = lat2 - lat1\n",
    "#     dlon = lon2 - lon1\n",
    "#     a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "#     c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "#     radius_of_earth = 6371000  # meters\n",
    "#     return radius_of_earth * c\n",
    "\n",
    "# # Group by 'osm_id', summing lengths and calculating the direct distance\n",
    "# def aggregate_osmid(group):\n",
    "#     total_length = group['length'].sum()\n",
    "#     first_point = Point(group.iloc[0]['first_point'])\n",
    "#     last_point = Point(group.iloc[-1]['last_point'])\n",
    "#     direct_length = calculate_direct_distance((first_point.x, first_point.y), (last_point.x, last_point.y))\n",
    "#     return pd.Series({'total_length': total_length, 'direct_length': direct_length})\n",
    "\n",
    "# # Group the data by 'osm_id' and apply aggregation\n",
    "# aggregated_data = network_detailed.groupby('osm_id').apply(aggregate_osmid)\n",
    "\n",
    "# # Display the aggregated results\n",
    "# print(aggregated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c880c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7e6744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f884b3d1",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ca2e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d57c2d8",
   "metadata": {},
   "source": [
    "# Get mobility data per ISRM for postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77988415",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "# ########################## INPUTS ###########################\n",
    "# #############################################################\n",
    "\n",
    "# # explodedPathTraversal['correct_length'] = explodedPathTraversal['linkLength'] / explodedPathTraversal['corr_VMT_by_county'] / scale                       \n",
    "   \n",
    "# # explodedPathTraversal['correct_length_bus'] = np.where(\n",
    "# #     explodedPathTraversal['vehicle_class_emfact'].isin(EMFACT_classes_bus), \n",
    "# #     explodedPathTraversal['correct_length'] * scale,          \n",
    "# #     explodedPathTraversal['correct_length']                    \n",
    "# # )\n",
    "\n",
    "# vmt_per_ISRM = explodedPathTraversal.groupby('ISRM').agg({'correct_length_bus':'sum'})\n",
    "\n",
    "# explodedPathTraversal['correct_duration'] = explodedPathTraversal['travelTime'] / explodedPathTraversal['corr_VMT_by_county'] / scale                       \n",
    "   \n",
    "# explodedPathTraversal['correct_duration_bus'] = np.where(\n",
    "#     explodedPathTraversal['vehicle_class_emfact'].isin(EMFACT_classes_bus), \n",
    "#     explodedPathTraversal['correct_duration'] * scale,          \n",
    "#     explodedPathTraversal['correct_duration']                    \n",
    "# )\n",
    "\n",
    "# vht_per_ISRM = explodedPathTraversal.groupby('ISRM').agg({'correct_duration_bus':'sum'})\n",
    "\n",
    "# mobility_stats = vmt_per_ISRM.merge(vht_per_ISRM, on = 'ISRM')\n",
    "# mobility_stats['av_speed'] = mobility_stats['correct_length_bus'] / mobility_stats['correct_duration_bus']\n",
    "# mobility_stats.to_csv(f'mobility_stats_{scenario}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac5dd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pollutants_speed_county_plot = ['tons_per_year_RUNEX_PM2_5', \n",
    "       'tons_per_year_RUNEX_SOx', \n",
    "       'tons_per_year_RUNEX_NOx', \n",
    "       'tons_per_year_RUNEX_ROG', \n",
    "       'tons_per_year_RUNEX_NH3', \n",
    "       'tons_per_year_PMBW_PM2_5', \n",
    "       'tons_per_year_PMTW_PM2_5', \n",
    "       'tons_per_year_RUNLOSS_ROG',]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "# Create a distinct color map for counties\n",
    "unique_counties = explodedPathTraversal['county_name'].unique()\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, len(unique_counties)))\n",
    "county_colors = {county: color for county, color in zip(unique_counties, colors)}\n",
    "\n",
    "# Process and plot data for each pollutant\n",
    "for i, pollutant in enumerate(pollutants_speed_county_plot):\n",
    "    # County distribution\n",
    "    county_distribution = (explodedPathTraversal.groupby('county_name')[pollutant].sum() /\n",
    "                           explodedPathTraversal[pollutant].sum()) * 100\n",
    "\n",
    "    # Plot county distribution with assigned colors\n",
    "    bottom = 0\n",
    "    for county, value in county_distribution.iteritems():\n",
    "        ax.bar(i, value, bottom=bottom, color=county_colors[county], width=0.8)\n",
    "        bottom += value\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xticks(range(len(pollutants_speed_county_plot)))\n",
    "ax.set_xticklabels([f'{p} - County' for p in pollutants_speed_county_plot], rotation=45, ha='right')\n",
    "ax.set_ylabel('Percentage Contribution (%)')\n",
    "ax.set_title('Contribution to Pollutants by County')\n",
    "\n",
    "# Create legend for counties\n",
    "patches = [plt.Rectangle((0, 0), 1, 1, color=county_colors[county]) for county in unique_counties]\n",
    "ax.legend(patches, unique_counties, title=\"Counties\", loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf1f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "# Create a color map for speeds\n",
    "unique_speeds = explodedPathTraversal['average_speed_mph'].unique()\n",
    "unique_speeds.sort()  # Optional: Sort speeds for a gradient effect\n",
    "colors = plt.cm.jet(np.linspace(0, 1, len(unique_speeds)))  # Using a different colormap for distinction\n",
    "speed_colors = {speed: color for speed, color in zip(unique_speeds, colors)}\n",
    "\n",
    "# Process and plot data for each pollutant\n",
    "for i, pollutant in enumerate(pollutants_speed_county_plot):\n",
    "    # Speed distribution\n",
    "    speed_distribution = (explodedPathTraversal.groupby('average_speed_mph')[pollutant].sum() /\n",
    "                          explodedPathTraversal[pollutant].sum()) * 100\n",
    "\n",
    "    # Plot speed distribution with assigned colors\n",
    "    bottom = 0\n",
    "    for speed, value in speed_distribution.iteritems():\n",
    "        ax.bar(i, value, bottom=bottom, color=speed_colors[speed], width=0.8)\n",
    "        bottom += value\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xticks(range(len(pollutants_speed_county_plot)))\n",
    "ax.set_xticklabels([f'{p} - Speed' for p in pollutants_speed_county_plot], rotation=45, ha='right')\n",
    "ax.set_ylabel('Percentage Contribution (%)')\n",
    "ax.set_title('Contribution to Pollutants by Average Speed')\n",
    "\n",
    "# Create legend for speeds\n",
    "patches = [plt.Rectangle((0, 0), 1, 1, color=speed_colors[speed]) for speed in unique_speeds]\n",
    "ax.legend(patches, [f'{speed} mph' for speed in unique_speeds], title=\"Average Speeds\", loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34be8e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089afd1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21673289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea080d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a360dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0229032c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49771932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbda015d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19c0d21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a568e59a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
